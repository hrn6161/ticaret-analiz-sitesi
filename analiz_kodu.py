import requests
from bs4 import BeautifulSoup
import time
import random
import re
from openpyxl import Workbook
from openpyxl.styles import Font
import sys
import logging
import os
from datetime import datetime

print("ğŸš€ GELÄ°ÅMÄ°Å AKILLI CRAWLER ANALÄ°Z SÄ°STEMÄ° BAÅLATILIYOR...")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

class Config:
    def __init__(self):
        self.MAX_RESULTS = 5  # Daha fazla sonuÃ§
        self.REQUEST_TIMEOUT = 20
        self.RETRY_ATTEMPTS = 3
        self.USER_AGENTS = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0",
        ]

class AdvancedCrawler:
    def __init__(self, config):
        self.config = config
        self.session = requests.Session()
    
    def advanced_crawl(self, url, target_country):
        """GeliÅŸmiÅŸ crawl - Ã§oklu teknikler"""
        print(f"   ğŸŒ GeliÅŸmiÅŸ crawl: {url[:80]}...")
        
        # 1. Deneme: Basit requests
        result = self._try_basic_request(url, target_country)
        if result['status_code'] == 200:
            return result
        
        # 2. Deneme: GeliÅŸmiÅŸ headers
        result = self._try_advanced_headers(url, target_country)
        if result['status_code'] == 200:
            return result
        
        # 3. Deneme: Session ile tekrar deneme
        result = self._try_with_session(url, target_country)
        if result['status_code'] == 200:
            return result
        
        # 4. Deneme: FarklÄ± User-Agent
        result = self._try_different_agent(url, target_country)
        if result['status_code'] == 200:
            return result
        
        print(f"   âš ï¸  TÃ¼m yÃ¶ntemler baÅŸarÄ±sÄ±z")
        return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'ALL_FAILED'}
    
    def _try_basic_request(self, url, target_country):
        """Basit request dene"""
        try:
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            }
            response = requests.get(url, headers=headers, timeout=15)
            if response.status_code == 200:
                return self._parse_advanced_content(response.text, target_country, response.status_code)
        except:
            pass
        return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'BASIC_FAILED'}
    
    def _try_advanced_headers(self, url, target_country):
        """GeliÅŸmiÅŸ headers ile dene"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
            }
            response = requests.get(url, headers=headers, timeout=15)
            if response.status_code == 200:
                return self._parse_advanced_content(response.text, target_country, response.status_code)
        except:
            pass
        return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'ADVANCED_FAILED'}
    
    def _try_with_session(self, url, target_country):
        """Session ile dene"""
        try:
            self.session.headers.update({
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            })
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                return self._parse_advanced_content(response.text, target_country, response.status_code)
        except:
            pass
        return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'SESSION_FAILED'}
    
    def _try_different_agent(self, url, target_country):
        """FarklÄ± User-Agent ile dene"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            }
            response = requests.get(url, headers=headers, timeout=15)
            if response.status_code == 200:
                return self._parse_advanced_content(response.text, target_country, response.status_code)
        except:
            pass
        return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'AGENT_FAILED'}
    
    def _parse_advanced_content(self, html, target_country, status_code):
        """GeliÅŸmiÅŸ iÃ§erik analizi"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Meta description'Ä± al
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            meta_content = meta_desc.get('content', '') if meta_desc else ''
            
            # Title'Ä± al
            title = soup.find('title')
            title_content = title.get_text() if title else ''
            
            # TÃ¼m metni al
            text_content = soup.get_text()
            combined_content = f"{title_content} {meta_content} {text_content}"
            
            # Temizleme
            lines = (line.strip() for line in combined_content.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            cleaned_content = ' '.join(chunk for chunk in chunks if chunk)
            
            text_lower = cleaned_content.lower()
            
            # Ãœlke ismini ara (farklÄ± varyasyonlarla)
            country_variations = [
                target_country.lower(),
                target_country.upper(),
                target_country.title()
            ]
            country_found = any(variation in text_lower for variation in country_variations)
            
            # GTIP/HS kodlarÄ±nÄ± ara
            gtip_codes = self.extract_advanced_gtip_codes(cleaned_content)
            
            content_preview = cleaned_content[:500] + "..." if len(cleaned_content) > 500 else cleaned_content
            
            print(f"   ğŸ” GeliÅŸmiÅŸ analiz: Ãœlke={country_found}, GTIP={gtip_codes}")
            
            return {
                'country_found': country_found,
                'gtip_codes': gtip_codes,
                'content_preview': content_preview,
                'status_code': status_code
            }
        except Exception as e:
            print(f"   âŒ Parse hatasÄ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'PARSE_ERROR'}
    
    def extract_advanced_gtip_codes(self, text):
        """GeliÅŸmiÅŸ GTIP kod Ã§Ä±karma"""
        patterns = [
            r'\b\d{4}\.?\d{0,4}\b',
            r'\bHS\s?CODE\s?:?\s?(\d{4,8})\b',
            r'\bHS\s?:?\s?(\d{4,8})\b',
            r'\bGTIP\s?:?\s?(\d{4,8})\b',
            r'\bH\.S\.\s?CODE?\s?:?\s?(\d{4,8})\b',
            r'\bHarmonized System\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bCustoms\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bTariff\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bProduct\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bItem\s?Code\s?:?\s?(\d{4,8})\b',
        ]
        
        all_codes = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                code = re.sub(r'[^\d]', '', match)
                if len(code) >= 4:
                    all_codes.add(code[:4])
        
        # 4-8 haneli sayÄ±larÄ± kontrol et (GTIP aralÄ±ÄŸÄ±nda mÄ±)
        number_pattern = r'\b\d{4,8}\b'
        numbers = re.findall(number_pattern, text)
        
        for num in numbers:
            if num.isdigit():
                num_int = int(num[:4])  # Ä°lk 4 haneye bak
                # GeniÅŸ GTIP aralÄ±klarÄ±
                gtip_ranges = [
                    (8400, 8600), (8700, 8900), (9000, 9300), 
                    (2800, 2900), (8470, 8480), (8500, 8520),
                    (8540, 8550), (9301, 9307), (4016, 4016),
                    (8708, 8708), (8542, 8542), (8471, 8471)
                ]
                for start, end in gtip_ranges:
                    if start <= num_int <= end:
                        all_codes.add(str(num_int))
                        break
        
        return list(all_codes)

class MultiSearcher:
    def __init__(self, config):
        self.config = config
    
    def comprehensive_search(self, query, max_results=5):
        """KapsamlÄ± arama - Ã§oklu sorgular"""
        all_results = []
        
        # DuckDuckGo arama
        ddg_results = self.search_duckduckgo(query, max_results)
        all_results.extend(ddg_results)
        
        # FarklÄ± sorgu varyasyonlarÄ±
        query_variations = self.generate_query_variations(query)
        for variation in query_variations[:2]:  # Ä°lk 2 varyasyon
            try:
                variation_results = self.search_duckduckgo(variation, max_results//2)
                all_results.extend(variation_results)
                time.sleep(1)  # Rate limiting
            except:
                pass
        
        # Benzersiz sonuÃ§lar
        unique_results = []
        seen_urls = set()
        
        for result in all_results:
            if result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)
        
        print(f"âœ… KapsamlÄ± arama: {len(unique_results)} benzersiz sonuÃ§")
        return unique_results[:max_results]
    
    def generate_query_variations(self, base_query):
        """Sorgu varyasyonlarÄ± oluÅŸtur"""
        variations = [
            f"{base_query}",
            f"{base_query} company",
            f"{base_query} trade",
            f"{base_query} export import",
            f"{base_query} business relations",
            f"{base_query} trading partner",
            f"{base_query} commercial",
            f'"{base_query}"',
        ]
        return variations
    
    def search_duckduckgo(self, query, max_results=5):
        """DuckDuckGo'da arama"""
        try:
            print(f"   ğŸ” DuckDuckGo: {query}")
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            }
            
            url = "https://html.duckduckgo.com/html/"
            data = {
                'q': query,
                'b': '',
                'kl': 'us-en'
            }
            
            response = requests.post(url, data=data, headers=headers, timeout=20)
            
            if response.status_code == 200:
                return self.parse_duckduckgo_results(response.text, max_results)
            else:
                print(f"   âŒ DuckDuckGo hatasÄ±: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"   âŒ DuckDuckGo arama hatasÄ±: {e}")
            return []
    
    def parse_duckduckgo_results(self, html, max_results):
        """DuckDuckGo sonuÃ§larÄ±nÄ± parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        result_divs = soup.find_all('div', class_='result')
        
        for div in result_divs[:max_results]:
            try:
                title_elem = div.find('a', class_='result__a')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                url = title_elem.get('href')
                
                # URL redirect
                if url and '//duckduckgo.com/l/' in url:
                    try:
                        redirect_response = requests.get(url, timeout=8, allow_redirects=True)
                        url = redirect_response.url
                    except:
                        pass
                
                snippet_elem = div.find('a', class_='result__snippet')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                if url and url.startswith('//'):
                    url = 'https:' + url
                
                results.append({
                    'title': title,
                    'url': url,
                    'snippet': snippet,
                    'full_text': f"{title} {snippet}",
                    'search_engine': 'duckduckgo'
                })
                
                print(f"      ğŸ“„ Bulunan: {title[:60]}...")
                
            except Exception as e:
                print(f"      âŒ SonuÃ§ parse hatasÄ±: {e}")
                continue
        
        return results

class EURLexChecker:
    def __init__(self, config):
        self.config = config
    
    def comprehensive_check_gtip(self, gtip_codes):
        """KapsamlÄ± GTIP kontrolÃ¼"""
        if not gtip_codes:
            return []
            
        sanctioned_codes = []
        
        for gtip_code in gtip_codes:
            try:
                print(f"   ğŸ” EUR-Lex kontrolÃ¼: GTIP {gtip_code}")
                
                time.sleep(random.uniform(1, 2))
                
                # FarklÄ± sorgular deneyelim
                search_terms = [
                    f'"{gtip_code}" prohibited banned sanction',
                    f'"{gtip_code}" restricted embargo',
                    f'"{gtip_code}" not allowed export',
                ]
                
                for search_term in search_terms:
                    url = "https://eur-lex.europa.eu/search.html"
                    params = {
                        'text': search_term,
                        'type': 'advanced',
                        'lang': 'en'
                    }
                    
                    headers = {
                        'User-Agent': random.choice(self.config.USER_AGENTS),
                    }
                    
                    response = requests.get(url, params=params, headers=headers, timeout=15)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        content = soup.get_text().lower()
                        
                        sanction_terms = [
                            'prohibited', 'banned', 'sanction', 'restricted', 
                            'embargo', 'forbidden', 'prohibition', 'ban',
                            'not allowed', 'not permitted', 'restriction'
                        ]
                        found_sanction = any(term in content for term in sanction_terms)
                        
                        if found_sanction:
                            sanctioned_codes.append(gtip_code)
                            print(f"   â›” YaptÄ±rÄ±mlÄ± kod bulundu: {gtip_code}")
                            break  # Bir kere bulduysa diÄŸerlerini kontrol etme
                        else:
                            print(f"   âœ… Kod temiz: {gtip_code}")
                            break
                    else:
                        print(f"   âŒ EUR-Lex eriÅŸim hatasÄ±: {response.status_code}")
                
            except Exception as e:
                print(f"   âŒ EUR-Lex kontrol hatasÄ± GTIP {gtip_code}: {e}")
                continue
        
        return sanctioned_codes

class ComprehensiveTradeAnalyzer:
    def __init__(self, config):
        self.config = config
        self.searcher = MultiSearcher(config)
        self.crawler = AdvancedCrawler(config)
        self.eur_lex_checker = EURLexChecker(config)
    
    def comprehensive_analyze(self, company, country):
        """KapsamlÄ± analiz"""
        print(f"ğŸ¤– KAPSAMLI ANALÄ°Z BAÅLATILIYOR: {company} â†” {country}")
        
        # Ã‡oklu arama sorgularÄ±
        search_queries = self.generate_search_queries(company, country)
        
        all_results = []
        
        for i, query in enumerate(search_queries, 1):
            try:
                print(f"\nğŸ” Sorgu {i}/{len(search_queries)}: {query}")
                
                search_results = self.searcher.comprehensive_search(query, self.config.MAX_RESULTS)
                
                if not search_results:
                    print(f"   âš ï¸ Bu sorgu iÃ§in sonuÃ§ bulunamadÄ±")
                    continue
                
                for j, result in enumerate(search_results, 1):
                    print(f"   ğŸ“„ SonuÃ§ {j} analiz ediliyor: {result['title'][:50]}...")
                    
                    # GeliÅŸmiÅŸ crawl
                    crawl_result = self.crawler.advanced_crawl(result['url'], country)
                    
                    # Snippet'ten GTIP Ã§Ä±kar
                    if not crawl_result['gtip_codes']:
                        snippet_gtips = self.crawler.extract_advanced_gtip_codes(result['full_text'])
                        if snippet_gtips:
                            crawl_result['gtip_codes'] = snippet_gtips
                            print(f"   ğŸ” Snippet'ten GTIP Ã§Ä±karÄ±ldÄ±: {snippet_gtips}")
                    
                    # EUR-Lex kontrolÃ¼
                    sanctioned_gtips = []
                    if crawl_result['gtip_codes']:
                        print(f"   ğŸ” EUR-Lex kontrolÃ¼ yapÄ±lÄ±yor...")
                        sanctioned_gtips = self.eur_lex_checker.comprehensive_check_gtip(crawl_result['gtip_codes'])
                    
                    # Analiz sonucu oluÅŸtur
                    analysis = self.create_analysis_result(
                        company, country, result, crawl_result, sanctioned_gtips
                    )
                    
                    all_results.append(analysis)
                
                # Sorgular arasÄ±nda bekleme
                if i < len(search_queries):
                    delay = random.uniform(2, 4)
                    print(f"   â³ {delay:.1f} saniye bekleniyor...")
                    time.sleep(delay)
                
            except Exception as e:
                print(f"   âŒ Sorgu hatasÄ±: {e}")
                continue
        
        return all_results
    
    def generate_search_queries(self, company, country):
        """Arama sorgularÄ± oluÅŸtur"""
        queries = [
            f"{company} {country} export",
            f"{company} {country} import",
            f"{company} {country} trade",
            f"{company} {country} business",
            f"{company} {country} trading partner",
            f"{company} {country} commercial relations",
            f"{company} {country} distributor",
            f"{company} {country} supplier",
            f'"{company}" "{country}"',
            f"{company} {country} HS code",
            f"{company} {country} GTIP",
        ]
        return queries
    
    def create_analysis_result(self, company, country, search_result, crawl_result, sanctioned_gtips):
        """Analiz sonucu oluÅŸtur"""
        
        if sanctioned_gtips:
            status = "YAPTIRIMLI_YÃœKSEK_RISK"
            explanation = f"â›” YÃœKSEK RÄ°SK: {company} ÅŸirketi {country} ile yaptÄ±rÄ±mlÄ± Ã¼rÃ¼n ticareti yapÄ±yor"
            ai_tavsiye = f"â›” ACÄ°L DURUM! Bu Ã¼rÃ¼nlerin {country.upper()} ihracÄ± YASAKTIR: {', '.join(sanctioned_gtips)}"
            risk_level = "YÃœKSEK"
        elif crawl_result['country_found'] and crawl_result['gtip_codes']:
            status = "RISK_VAR"
            explanation = f"ğŸŸ¡ RÄ°SK VAR: {company} ÅŸirketi {country} ile ticaret baÄŸlantÄ±sÄ± bulundu"
            ai_tavsiye = f"Ticaret baÄŸlantÄ±sÄ± doÄŸrulandÄ±. GTIP kodlarÄ±: {', '.join(crawl_result['gtip_codes'])}"
            risk_level = "ORTA"
        elif crawl_result['country_found']:
            status = "Ä°LÄ°ÅKÄ°_VAR"
            explanation = f"ğŸŸ¢ Ä°LÄ°ÅKÄ° VAR: {company} ÅŸirketi {country} ile baÄŸlantÄ±lÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulundu ancak GTIP kodu tespit edilemedi"
            risk_level = "DÃœÅÃœK"
        else:
            status = "TEMIZ"
            explanation = f"âœ… TEMÄ°Z: {company} ÅŸirketinin {country} ile ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            risk_level = "YOK"
        
        return {
            'ÅÄ°RKET': company,
            'ÃœLKE': country,
            'DURUM': status,
            'AI_AÃ‡IKLAMA': explanation,
            'AI_TAVSIYE': ai_tavsiye,
            'YAPTIRIM_RISKI': risk_level,
            'TESPIT_EDILEN_GTIPLER': ', '.join(crawl_result['gtip_codes']),
            'YAPTIRIMLI_GTIPLER': ', '.join(sanctioned_gtips),
            'ULKE_BAGLANTISI': 'EVET' if crawl_result['country_found'] else 'HAYIR',
            'BAÅLIK': search_result['title'],
            'URL': search_result['url'],
            'Ã–ZET': search_result['snippet'],
            'CONTENT_PREVIEW': crawl_result['content_preview'],
            'STATUS_CODE': crawl_result.get('status_code', 'N/A'),
            'CRAWLER_TIPI': 'GELÄ°ÅMÄ°Å_CRAWLER',
            'ARAMA_MOTORU': search_result.get('search_engine', 'duckduckgo')
        }

def create_comprehensive_excel_report(results, company, country):
    """KapsamlÄ± Excel raporu"""
    try:
        filename = f"{company.replace(' ', '_')}_{country}_kapsamli_analiz.xlsx"
        
        wb = Workbook()
        ws = wb.active
        ws.title = "KapsamlÄ± Analiz SonuÃ§larÄ±"
        
        headers = [
            'ÅÄ°RKET', 'ÃœLKE', 'DURUM', 'YAPTIRIM_RISKI', 'ULKE_BAGLANTISI',
            'TESPIT_EDILEN_GTIPLER', 'YAPTIRIMLI_GTIPLER', 'AI_AÃ‡IKLAMA',
            'AI_TAVSIYE', 'BAÅLIK', 'URL', 'CRAWLER_TIPI', 'ARAMA_MOTORU'
        ]
        
        for col, header in enumerate(headers, 1):
            ws.cell(row=1, column=col, value=header).font = Font(bold=True)
        
        for row, result in enumerate(results, 2):
            ws.cell(row=row, column=1, value=str(result.get('ÅÄ°RKET', '')))
            ws.cell(row=row, column=2, value=str(result.get('ÃœLKE', '')))
            ws.cell(row=row, column=3, value=str(result.get('DURUM', '')))
            ws.cell(row=row, column=4, value=str(result.get('YAPTIRIM_RISKI', '')))
            ws.cell(row=row, column=5, value=str(result.get('ULKE_BAGLANTISI', '')))
            ws.cell(row=row, column=6, value=str(result.get('TESPIT_EDILEN_GTIPLER', '')))
            ws.cell(row=row, column=7, value=str(result.get('YAPTIRIMLI_GTIPLER', '')))
            ws.cell(row=row, column=8, value=str(result.get('AI_AÃ‡IKLAMA', '')))
            ws.cell(row=row, column=9, value=str(result.get('AI_TAVSIYE', '')))
            ws.cell(row=row, column=10, value=str(result.get('BAÅLIK', '')))
            ws.cell(row=row, column=11, value=str(result.get('URL', '')))
            ws.cell(row=row, column=12, value=str(result.get('CRAWLER_TIPI', '')))
            ws.cell(row=row, column=13, value=str(result.get('ARAMA_MOTORU', '')))
        
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width
        
        wb.save(filename)
        print(f"âœ… Excel raporu oluÅŸturuldu: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ Excel rapor oluÅŸturma hatasÄ±: {e}")
        return None

def display_results(results, company, country):
    """SonuÃ§larÄ± ekranda gÃ¶ster"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š KAPSAMLI ANALÄ°Z SONUÃ‡LARI: {company} â†” {country}")
    print(f"{'='*80}")
    
    if not results:
        print("âŒ Analiz sonucu bulunamadÄ±!")
        return
    
    total_results = len(results)
    high_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'YÃœKSEK'])
    medium_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'ORTA'])
    country_connection_count = len([r for r in results if r.get('ULKE_BAGLANTISI') == 'EVET'])
    
    print(f"\nğŸ“ˆ Ã–ZET:")
    print(f"   â€¢ Toplam SonuÃ§: {total_results}")
    print(f"   â€¢ Ãœlke BaÄŸlantÄ±sÄ±: {country_connection_count}")
    print(f"   â€¢ YÃœKSEK YaptÄ±rÄ±m Riski: {high_risk_count}")
    print(f"   â€¢ ORTA Risk: {medium_risk_count}")
    print(f"   â€¢ Crawler Tipi: GELÄ°ÅMÄ°Å CRAWLER (Ã‡oklu Teknik)")
    
    if high_risk_count > 0:
        print(f"\nâš ï¸  KRÄ°TÄ°K YAPTIRIM UYARISI:")
        for result in results:
            if result.get('YAPTIRIM_RISKI') == 'YÃœKSEK':
                print(f"   ğŸ”´ {result.get('BAÅLIK', '')[:60]}...")
                print(f"      YasaklÄ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
    
    for i, result in enumerate(results, 1):
        print(f"\nğŸ” SONUÃ‡ {i}:")
        print(f"   ğŸ“ BaÅŸlÄ±k: {result.get('BAÅLIK', 'N/A')}")
        print(f"   ğŸŒ URL: {result.get('URL', 'N/A')}")
        print(f"   ğŸ“‹ Ã–zet: {result.get('Ã–ZET', 'N/A')[:100]}...")
        print(f"   ğŸ¯ Durum: {result.get('DURUM', 'N/A')}")
        print(f"   âš ï¸  YaptÄ±rÄ±m Riski: {result.get('YAPTIRIM_RISKI', 'N/A')}")
        print(f"   ğŸ”— Ãœlke BaÄŸlantÄ±sÄ±: {result.get('ULKE_BAGLANTISI', 'N/A')}")
        print(f"   ğŸ” GTIP KodlarÄ±: {result.get('TESPIT_EDILEN_GTIPLER', 'Yok')}")
        if result.get('YAPTIRIMLI_GTIPLER'):
            print(f"   ğŸš« YasaklÄ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
        print(f"   ğŸ“Š Status Code: {result.get('STATUS_CODE', 'N/A')}")
        print(f"   ğŸ¤– Crawler: {result.get('CRAWLER_TIPI', 'N/A')}")
        print(f"   ğŸ” Arama Motoru: {result.get('ARAMA_MOTORU', 'N/A')}")
        print(f"   ğŸ’¡ AÃ§Ä±klama: {result.get('AI_AÃ‡IKLAMA', 'N/A')}")
        print(f"   ğŸ’­ Tavsiye: {result.get('AI_TAVSIYE', 'N/A')}")
        print(f"   {'â”€'*60}")

def main():
    print("ğŸ“Š GELÄ°ÅMÄ°Å CRAWLER ANALÄ°Z SÄ°STEMÄ°")
    print("ğŸ¯ Ã–ZELLÄ°K: Ã‡oklu Arama + GeliÅŸmiÅŸ Crawling + KapsamlÄ± Analiz")
    print("ğŸ’¡ AVANTAJ: 11 farklÄ± sorgu, 5 sonuÃ§, Ã§oklu site giriÅŸi")
    print("ğŸš€ HEDEF: Maksimum kapsam ve derinlik\n")
    
    # YapÄ±landÄ±rma
    config = Config()
    analyzer = ComprehensiveTradeAnalyzer(config)
    
    # Manuel giriÅŸ
    company = input("Åirket adÄ±nÄ± girin: ").strip()
    country = input("Ãœlke adÄ±nÄ± girin: ").strip()
    
    if not company or not country:
        print("âŒ Åirket ve Ã¼lke bilgisi gereklidir!")
        return
    
    print(f"\nğŸš€ KAPSAMLI ANALÄ°Z BAÅLATILIYOR: {company} â†” {country}")
    print("â³ 11 farklÄ± sorgu ile kapsamlÄ± arama yapÄ±lÄ±yor...")
    print("   GeliÅŸmiÅŸ crawler ile Ã§oklu site analizi...")
    print("   EUR-Lex ile kapsamlÄ± yaptÄ±rÄ±m kontrolÃ¼...\n")
    
    start_time = time.time()
    results = analyzer.comprehensive_analyze(company, country)
    execution_time = time.time() - start_time
    
    if results:
        # SonuÃ§larÄ± gÃ¶ster
        display_results(results, company, country)
        
        # Excel raporu oluÅŸtur
        filename = create_comprehensive_excel_report(results, company, country)
        
        if filename:
            print(f"\nâœ… Excel raporu oluÅŸturuldu: {filename}")
            print(f"â±ï¸  Toplam Ã§alÄ±ÅŸma sÃ¼resi: {execution_time:.2f} saniye")
            
            if execution_time > 30:
                print("âš ï¸  UYARI: Analiz 30 saniyeyi aÅŸtÄ±, Render'da timeout riski var!")
            else:
                print("âœ… BAÅARILI: Analiz 30 saniye altÄ±nda tamamlandÄ±!")
            
            # Excel aÃ§ma seÃ§eneÄŸi
            try:
                open_excel = input("\nğŸ“‚ Excel dosyasÄ±nÄ± ÅŸimdi aÃ§mak ister misiniz? (e/h): ").strip().lower()
                if open_excel == 'e':
                    if os.name == 'nt':  # Windows
                        os.system(f'start excel "{filename}"')
                    elif os.name == 'posix':  # macOS/Linux
                        os.system(f'open "{filename}"' if sys.platform == 'darwin' else f'xdg-open "{filename}"')
                    print("ğŸ“‚ Excel dosyasÄ± aÃ§Ä±lÄ±yor...")
            except Exception as e:
                print(f"âš ï¸  Dosya otomatik aÃ§Ä±lamadÄ±: {e}")
                print(f"ğŸ“ LÃ¼tfen manuel olarak aÃ§Ä±n: {filename}")
        else:
            print("âŒ Excel raporu oluÅŸturulamadÄ±!")
    else:
        print("âŒ Analiz sonucu bulunamadÄ±!")

if __name__ == "__main__":
    main()
