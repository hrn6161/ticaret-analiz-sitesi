import requests
from bs4 import BeautifulSoup
import time
import random
import re
from openpyxl import Workbook
from openpyxl.styles import Font
import sys
import logging
import os
from datetime import datetime

print("ğŸš€ GELÄ°ÅMÄ°Å CRAWLER Ä°LE DUCKDUCKGO ANALÄ°Z SÄ°STEMÄ° BAÅLATILIYOR...")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

class Config:
    def __init__(self):
        self.MAX_RESULTS = 3
        self.REQUEST_TIMEOUT = 30
        self.RETRY_ATTEMPTS = 3
        self.DELAY_BETWEEN_REQUESTS = random.uniform(3, 7)  # Rastgele bekleme 3-7 saniye
        self.DELAY_BETWEEN_SEARCHES = random.uniform(5, 10)  # Rastgele bekleme 5-10 saniye
        self.USER_AGENTS = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/120.0",
        ]

class AdvancedCrawler:
    def __init__(self, config):
        self.config = config
    
    def crawl_with_retry(self, url, target_country, max_retries=3):
        """SayfayÄ± crawl et - retry mekanizmasÄ± ile"""
        for attempt in range(max_retries):
            try:
                # Rastgele bekleme (anti-bot iÃ§in)
                delay = random.uniform(2, 5)
                print(f"   â³ {delay:.1f} saniye bekleniyor (attempt {attempt + 1}/{max_retries})...")
                time.sleep(delay)
                
                return self._crawl_page(url, target_country)
                
            except Exception as e:
                print(f"   âš ï¸ Crawl attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    retry_delay = random.uniform(5, 10)
                    print(f"   ğŸ”„ {retry_delay:.1f} saniye sonra tekrar denenecek...")
                    time.sleep(retry_delay)
                else:
                    print(f"   âŒ TÃ¼m crawl attempt'leri baÅŸarÄ±sÄ±z: {e}")
        
        return {'country_found': False, 'gtip_codes': [], 'content_preview': ''}
    
    def _crawl_page(self, url, target_country):
        """Sayfa iÃ§eriÄŸini crawl et"""
        try:
            if not url or not url.startswith('http'):
                return {'country_found': False, 'gtip_codes': [], 'content_preview': ''}
                
            print(f"   ğŸŒ Sayfa crawl ediliyor: {url}")
            
            # Rastgele user-agent seÃ§
            user_agent = random.choice(self.config.USER_AGENTS)
            
            headers = {
                'User-Agent': user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
            }
            
            # Session kullanarak daha gerÃ§ekÃ§i tarama
            session = requests.Session()
            session.headers.update(headers)
            
            response = session.get(
                url, 
                timeout=self.config.REQUEST_TIMEOUT,
                allow_redirects=True
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Script ve style tag'lerini temizle
                for script in soup(["script", "style", "nav", "header", "footer"]):
                    script.decompose()
                
                # Meta description'Ä± da al
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                meta_content = meta_desc.get('content', '') if meta_desc else ''
                
                # TÃ¼m metni al
                text_content = soup.get_text()
                combined_content = f"{meta_content} {text_content}"
                
                # Temizleme
                lines = (line.strip() for line in combined_content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                cleaned_content = ' '.join(chunk for chunk in chunks if chunk)
                
                text_lower = cleaned_content.lower()
                
                # Ãœlke ismini ara (case insensitive)
                country_found = target_country.lower() in text_lower
                
                # GTIP/HS kodlarÄ±nÄ± ara
                gtip_codes = self.extract_gtip_codes(cleaned_content)
                
                content_preview = cleaned_content[:400] + "..." if len(cleaned_content) > 400 else cleaned_content
                
                print(f"   ğŸ” Sayfa analizi: Ãœlke bulundu={country_found}, GTIP kodlarÄ±={gtip_codes}")
                
                return {
                    'country_found': country_found,
                    'gtip_codes': gtip_codes,
                    'content_preview': content_preview,
                    'status_code': response.status_code
                }
            else:
                print(f"   âŒ Sayfa crawl hatasÄ±: {response.status_code} - {url}")
                return {
                    'country_found': False, 
                    'gtip_codes': [], 
                    'content_preview': '',
                    'status_code': response.status_code
                }
                
        except Exception as e:
            print(f"   âŒ Crawl hatasÄ± {url}: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': ''}
    
    def extract_gtip_codes(self, text):
        """Metinden GTIP/HS kodlarÄ±nÄ± Ã§Ä±kar"""
        patterns = [
            r'\b\d{4}\.?\d{0,4}\b',
            r'\bHS\s?CODE\s?:?\s?(\d{4,8})\b',
            r'\bHS\s?:?\s?(\d{4,8})\b',
            r'\bGTIP\s?:?\s?(\d{4,8})\b',
            r'\bH\.S\.\s?CODE?\s?:?\s?(\d{4,8})\b',
            r'\bHarmonized System\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bCustoms\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bTariff\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bCN\s?code\s?:?\s?(\d{4,8})\b',
        ]
        
        all_codes = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                
                code = re.sub(r'[^\d]', '', match)
                if len(code) >= 4:
                    main_code = code[:4]
                    if main_code.isdigit():
                        all_codes.add(main_code)
        
        # 4 haneli sayÄ±larÄ± kontrol et (GTIP aralÄ±ÄŸÄ±nda mÄ±)
        number_pattern = r'\b\d{4}\b'
        numbers = re.findall(number_pattern, text)
        
        for num in numbers:
            if num.isdigit():
                num_int = int(num)
                # GeniÅŸ GTIP aralÄ±klarÄ±
                if ((8400 <= num_int <= 8600) or (8700 <= num_int <= 8900) or 
                    (9000 <= num_int <= 9300) or (2800 <= num_int <= 2900) or
                    (8470 <= num_int <= 8480) or (8500 <= num_int <= 8520) or
                    (8540 <= num_int <= 8550) or (9301 <= num_int <= 9307) or
                    (8701 <= num_int <= 8716) or (8801 <= num_int <= 8807)):
                    all_codes.add(num)
        
        return list(all_codes)

class DuckDuckGoSearcher:
    def __init__(self, config):
        self.config = config
    
    def search_duckduckgo(self, query, max_results=3):
        """DuckDuckGo'da arama yap"""
        try:
            print(f"ğŸ” DuckDuckGo'da aranÄ±yor: {query}")
            
            # TÄ±rnak iÅŸaretlerini kaldÄ±r
            query = query.replace('"', '')
            
            # Rastgele bekleme
            delay = random.uniform(2, 4)
            time.sleep(delay)
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
            }
            
            # DuckDuckGo HTML arama URL'si
            url = "https://html.duckduckgo.com/html/"
            data = {
                'q': query,
                'b': '',
                'kl': 'us-en'
            }
            
            response = requests.post(
                url, 
                data=data, 
                headers=headers, 
                timeout=self.config.REQUEST_TIMEOUT
            )
            
            if response.status_code == 200:
                return self.parse_duckduckgo_results(response.text, max_results)
            else:
                print(f"âŒ DuckDuckGo arama hatasÄ±: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ DuckDuckGo arama hatasÄ±: {e}")
            return []
    
    def parse_duckduckgo_results(self, html, max_results):
        """DuckDuckGo sonuÃ§larÄ±nÄ± parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        # DuckDuckGo search result div'leri
        result_divs = soup.find_all('div', class_='result')
        
        for div in result_divs[:max_results]:
            try:
                # BaÅŸlÄ±k
                title_elem = div.find('a', class_='result__a')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                
                # URL
                url = title_elem.get('href')
                if url and '//duckduckgo.com/l/' in url:
                    # DuckDuckGo redirect link'ini Ã§Ã¶z
                    try:
                        redirect_response = requests.get(url, timeout=10, allow_redirects=True)
                        url = redirect_response.url
                    except:
                        # Redirect Ã§Ã¶zÃ¼lemezse orijinal URL'yi kullan
                        pass
                
                # Ã–zet
                snippet_elem = div.find('a', class_='result__snippet')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                # URL'yi temizle
                if url and url.startswith('//'):
                    url = 'https:' + url
                
                results.append({
                    'title': title,
                    'url': url,
                    'snippet': snippet
                })
                
                print(f"   ğŸ“„ Bulunan sonuÃ§: {title[:60]}...")
                
            except Exception as e:
                print(f"   âŒ SonuÃ§ parse hatasÄ±: {e}")
                continue
        
        print(f"âœ… DuckDuckGo'dan {len(results)} sonuÃ§ bulundu")
        return results

class EURLexChecker:
    def __init__(self, config):
        self.config = config
    
    def check_gtip_in_eur_lex(self, gtip_codes):
        """GTIP kodlarÄ±nÄ± EUR-Lex'te kontrol et"""
        sanctioned_codes = []
        
        for gtip_code in gtip_codes:
            try:
                print(f"   ğŸ” EUR-Lex'te kontrol ediliyor: GTIP {gtip_code}")
                
                # Rastgele bekleme
                delay = random.uniform(1, 3)
                time.sleep(delay)
                
                # EUR-Lex arama URL'si
                url = "https://eur-lex.europa.eu/search.html"
                params = {
                    'qid': int(time.time()),
                    'text': f'"{gtip_code}" prohibited banned sanction restricted embargo',
                    'type': 'advanced',
                    'lang': 'en'
                }
                
                headers = {
                    'User-Agent': random.choice(self.config.USER_AGENTS),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                }
                
                response = requests.get(url, params=params, headers=headers, timeout=self.config.REQUEST_TIMEOUT)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content = soup.get_text().lower()
                    
                    # YaptÄ±rÄ±m terimlerini ara
                    sanction_terms = [
                        'prohibited', 'banned', 'sanction', 'restricted', 
                        'embargo', 'forbidden', 'prohibition', 'ban',
                        'not allowed', 'not permitted', 'restriction'
                    ]
                    found_sanction = any(term in content for term in sanction_terms)
                    
                    if found_sanction:
                        sanctioned_codes.append(gtip_code)
                        print(f"   â›” YaptÄ±rÄ±mlÄ± kod bulundu: {gtip_code}")
                    else:
                        print(f"   âœ… Kod temiz: {gtip_code}")
                else:
                    print(f"   âŒ EUR-Lex eriÅŸim hatasÄ±: {response.status_code}")
                
            except Exception as e:
                print(f"   âŒ EUR-Lex kontrol hatasÄ± GTIP {gtip_code}: {e}")
                continue
        
        return sanctioned_codes

class AdvancedTradeAnalyzer:
    def __init__(self, config):
        self.config = config
        self.searcher = DuckDuckGoSearcher(config)
        self.crawler = AdvancedCrawler(config)
        self.eur_lex_checker = EURLexChecker(config)
    
    def analyze_company_country(self, company, country):
        """Åirket-Ã¼lke analizi yap"""
        print(f"ğŸ¤– GELÄ°ÅMÄ°Å ANALÄ°Z BAÅLATILIYOR: {company} â†” {country}")
        
        search_queries = [
            f"{company} {country} export",
            f"{company} {country} business",
            f"{company} {country} trade",
        ]
        
        all_results = []
        
        for i, query in enumerate(search_queries, 1):
            try:
                print(f"\nğŸ” Sorgu {i}/{len(search_queries)}: {query}")
                
                # DuckDuckGo'da ara
                search_results = self.searcher.search_duckduckgo(query, self.config.MAX_RESULTS)
                
                if not search_results:
                    print(f"   âš ï¸ Bu sorgu iÃ§in sonuÃ§ bulunamadÄ±")
                    continue
                
                for j, result in enumerate(search_results, 1):
                    print(f"   ğŸ“„ SonuÃ§ {j} analiz ediliyor: {result['title'][:50]}...")
                    
                    # SayfayÄ± crawl et (retry ile)
                    crawl_result = self.crawler.crawl_with_retry(result['url'], country)
                    
                    # EÄŸer Ã¼lke baÄŸlantÄ±sÄ± bulunduysa ve GTIP kodlarÄ± varsa, EUR-Lex kontrolÃ¼ yap
                    sanctioned_gtips = []
                    if crawl_result['country_found'] and crawl_result['gtip_codes']:
                        print(f"   ğŸ” EUR-Lex kontrolÃ¼ yapÄ±lÄ±yor...")
                        sanctioned_gtips = self.eur_lex_checker.check_gtip_in_eur_lex(crawl_result['gtip_codes'])
                    
                    # Analiz sonucu oluÅŸtur
                    analysis = self.create_analysis_result(
                        company, country, result, crawl_result, sanctioned_gtips
                    )
                    
                    all_results.append(analysis)
                
                # Sorgular arasÄ±nda rastgele bekleme
                if i < len(search_queries):
                    delay = random.uniform(5, 10)
                    print(f"   â³ {delay:.1f} saniye bekleniyor (sorgular arasÄ±)...")
                    time.sleep(delay)
                
            except Exception as e:
                print(f"   âŒ Sorgu hatasÄ±: {e}")
                continue
        
        return all_results
    
    def create_analysis_result(self, company, country, search_result, crawl_result, sanctioned_gtips):
        """Analiz sonucu oluÅŸtur"""
        
        # Risk deÄŸerlendirmesi
        if sanctioned_gtips:
            status = "YAPTIRIMLI_YÃœKSEK_RISK"
            explanation = f"â›” YÃœKSEK RÄ°SK: {company} ÅŸirketi {country} ile yaptÄ±rÄ±mlÄ± Ã¼rÃ¼n ticareti yapÄ±yor"
            ai_tavsiye = f"â›” ACÄ°L DURUM! Bu Ã¼rÃ¼nlerin {country.upper()} ihracÄ± YASAKTIR: {', '.join(sanctioned_gtips)}"
            risk_level = "YÃœKSEK"
        elif crawl_result['country_found'] and crawl_result['gtip_codes']:
            status = "RISK_VAR"
            explanation = f"ğŸŸ¡ RÄ°SK VAR: {company} ÅŸirketi {country} ile ticaret baÄŸlantÄ±sÄ± bulundu"
            ai_tavsiye = f"Ticaret baÄŸlantÄ±sÄ± doÄŸrulandÄ±. GTIP kodlarÄ±: {', '.join(crawl_result['gtip_codes'])}"
            risk_level = "ORTA"
        elif crawl_result['country_found']:
            status = "Ä°LÄ°ÅKÄ°_VAR"
            explanation = f"ğŸŸ¢ Ä°LÄ°ÅKÄ° VAR: {company} ÅŸirketi {country} ile baÄŸlantÄ±lÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulundu ancak GTIP kodu tespit edilemedi"
            risk_level = "DÃœÅÃœK"
        else:
            status = "TEMIZ"
            explanation = f"âœ… TEMÄ°Z: {company} ÅŸirketinin {country} ile ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            risk_level = "YOK"
        
        return {
            'ÅÄ°RKET': company,
            'ÃœLKE': country,
            'DURUM': status,
            'AI_AÃ‡IKLAMA': explanation,
            'AI_TAVSIYE': ai_tavsiye,
            'YAPTIRIM_RISKI': risk_level,
            'TESPIT_EDILEN_GTIPLER': ', '.join(crawl_result['gtip_codes']),
            'YAPTIRIMLI_GTIPLER': ', '.join(sanctioned_gtips),
            'ULKE_BAGLANTISI': 'EVET' if crawl_result['country_found'] else 'HAYIR',
            'BAÅLIK': search_result['title'],
            'URL': search_result['url'],
            'Ã–ZET': search_result['snippet'],
            'CONTENT_PREVIEW': crawl_result['content_preview'],
            'STATUS_CODE': crawl_result.get('status_code', 'N/A')
        }

def create_excel_report(results, company, country):
    """Excel raporu oluÅŸtur"""
    try:
        filename = f"{company.replace(' ', '_')}_{country}_gelismis_analiz.xlsx"
        
        wb = Workbook()
        ws = wb.active
        ws.title = "GeliÅŸmiÅŸ Analiz SonuÃ§larÄ±"
        
        headers = [
            'ÅÄ°RKET', 'ÃœLKE', 'DURUM', 'YAPTIRIM_RISKI', 'ULKE_BAGLANTISI',
            'TESPIT_EDILEN_GTIPLER', 'YAPTIRIMLI_GTIPLER', 'AI_AÃ‡IKLAMA',
            'AI_TAVSIYE', 'BAÅLIK', 'URL', 'Ã–ZET', 'STATUS_CODE'
        ]
        
        for col, header in enumerate(headers, 1):
            ws.cell(row=1, column=col, value=header).font = Font(bold=True)
        
        for row, result in enumerate(results, 2):
            ws.cell(row=row, column=1, value=str(result.get('ÅÄ°RKET', '')))
            ws.cell(row=row, column=2, value=str(result.get('ÃœLKE', '')))
            ws.cell(row=row, column=3, value=str(result.get('DURUM', '')))
            ws.cell(row=row, column=4, value=str(result.get('YAPTIRIM_RISKI', '')))
            ws.cell(row=row, column=5, value=str(result.get('ULKE_BAGLANTISI', '')))
            ws.cell(row=row, column=6, value=str(result.get('TESPIT_EDILEN_GTIPLER', '')))
            ws.cell(row=row, column=7, value=str(result.get('YAPTIRIMLI_GTIPLER', '')))
            ws.cell(row=row, column=8, value=str(result.get('AI_AÃ‡IKLAMA', '')))
            ws.cell(row=row, column=9, value=str(result.get('AI_TAVSIYE', '')))
            ws.cell(row=row, column=10, value=str(result.get('BAÅLIK', '')))
            ws.cell(row=row, column=11, value=str(result.get('URL', '')))
            ws.cell(row=row, column=12, value=str(result.get('Ã–ZET', '')))
            ws.cell(row=row, column=13, value=str(result.get('STATUS_CODE', '')))
        
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width
        
        wb.save(filename)
        print(f"âœ… Excel raporu oluÅŸturuldu: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ Excel rapor oluÅŸturma hatasÄ±: {e}")
        return None

def display_results(results, company, country):
    """SonuÃ§larÄ± ekranda gÃ¶ster"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š GELÄ°ÅMÄ°Å ANALÄ°Z SONUÃ‡LARI: {company} â†” {country}")
    print(f"{'='*80}")
    
    if not results:
        print("âŒ Analiz sonucu bulunamadÄ±!")
        return
    
    total_results = len(results)
    high_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'YÃœKSEK'])
    medium_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'ORTA'])
    country_connection_count = len([r for r in results if r.get('ULKE_BAGLANTISI') == 'EVET'])
    
    print(f"\nğŸ“ˆ Ã–ZET:")
    print(f"   â€¢ Toplam SonuÃ§: {total_results}")
    print(f"   â€¢ Ãœlke BaÄŸlantÄ±sÄ±: {country_connection_count}")
    print(f"   â€¢ YÃœKSEK YaptÄ±rÄ±m Riski: {high_risk_count}")
    print(f"   â€¢ ORTA Risk: {medium_risk_count}")
    
    if high_risk_count > 0:
        print(f"\nâš ï¸  KRÄ°TÄ°K YAPTIRIM UYARISI:")
        for result in results:
            if result.get('YAPTIRIM_RISKI') == 'YÃœKSEK':
                print(f"   ğŸ”´ {result.get('BAÅLIK', '')[:60]}...")
                print(f"      YasaklÄ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
    
    for i, result in enumerate(results, 1):
        print(f"\nğŸ” SONUÃ‡ {i}:")
        print(f"   ğŸ“ BaÅŸlÄ±k: {result.get('BAÅLIK', 'N/A')}")
        print(f"   ğŸŒ URL: {result.get('URL', 'N/A')}")
        print(f"   ğŸ“‹ Ã–zet: {result.get('Ã–ZET', 'N/A')[:100]}...")
        print(f"   ğŸ¯ Durum: {result.get('DURUM', 'N/A')}")
        print(f"   âš ï¸  YaptÄ±rÄ±m Riski: {result.get('YAPTIRIM_RISKI', 'N/A')}")
        print(f"   ğŸ”— Ãœlke BaÄŸlantÄ±sÄ±: {result.get('ULKE_BAGLANTISI', 'N/A')}")
        print(f"   ğŸ” GTIP KodlarÄ±: {result.get('TESPIT_EDILEN_GTIPLER', 'Yok')}")
        if result.get('YAPTIRIMLI_GTIPLER'):
            print(f"   ğŸš« YasaklÄ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
        print(f"   ğŸ“Š Status Code: {result.get('STATUS_CODE', 'N/A')}")
        print(f"   ğŸ’¡ AÃ§Ä±klama: {result.get('AI_AÃ‡IKLAMA', 'N/A')}")
        print(f"   ğŸ’­ Tavsiye: {result.get('AI_TAVSIYE', 'N/A')}")
        print(f"   {'â”€'*60}")

def main():
    print("ğŸ“Š GELÄ°ÅMÄ°Å CRAWLER Ä°LE DUCKDUCKGO ANALÄ°Z SÄ°STEMÄ°")
    print("ğŸŒ Ã–ZELLÄ°K: Retry MekanizmasÄ± + Rastgele Delay + User-Agent Rotasyonu")
    print("ğŸ’¡ NOT: Bu versiyon 403 hatalarÄ±na raÄŸmen sitelere girmeyi deneyecek!\n")
    
    # YapÄ±landÄ±rma
    config = Config()
    analyzer = AdvancedTradeAnalyzer(config)
    
    # Manuel giriÅŸ
    company = input("Åirket adÄ±nÄ± girin: ").strip()
    country = input("Ãœlke adÄ±nÄ± girin: ").strip()
    
    if not company or not country:
        print("âŒ Åirket ve Ã¼lke bilgisi gereklidir!")
        return
    
    print(f"\nğŸš€ GELÄ°ÅMÄ°Å ANALÄ°Z BAÅLATILIYOR: {company} â†” {country}")
    print("â³ GerÃ§ek aramalar yapÄ±lÄ±yor, lÃ¼tfen bekleyin...")
    print("   Bu iÅŸlem 403 hatalarÄ±na raÄŸmen sitelere girmeyi deneyecek!")
    print("   Rastgele bekleme sÃ¼releri nedeniyle biraz zaman alabilir...\n")
    
    start_time = time.time()
    results = analyzer.analyze_company_country(company, country)
    execution_time = time.time() - start_time
    
    if results:
        # SonuÃ§larÄ± gÃ¶ster
        display_results(results, company, country)
        
        # Excel raporu oluÅŸtur
        filename = create_excel_report(results, company, country)
        
        if filename:
            print(f"\nâœ… Excel raporu oluÅŸturuldu: {filename}")
            print(f"â±ï¸  Toplam Ã§alÄ±ÅŸma sÃ¼resi: {execution_time:.2f} saniye")
            
            # Excel aÃ§ma seÃ§eneÄŸi
            try:
                open_excel = input("\nğŸ“‚ Excel dosyasÄ±nÄ± ÅŸimdi aÃ§mak ister misiniz? (e/h): ").strip().lower()
                if open_excel == 'e':
                    if os.name == 'nt':  # Windows
                        os.system(f'start excel "{filename}"')
                    elif os.name == 'posix':  # macOS/Linux
                        os.system(f'open "{filename}"' if sys.platform == 'darwin' else f'xdg-open "{filename}"')
                    print("ğŸ“‚ Excel dosyasÄ± aÃ§Ä±lÄ±yor...")
            except Exception as e:
                print(f"âš ï¸  Dosya otomatik aÃ§Ä±lamadÄ±: {e}")
                print(f"ğŸ“ LÃ¼tfen manuel olarak aÃ§Ä±n: {filename}")
        else:
            print("âŒ Excel raporu oluÅŸturulamadÄ±!")
    else:
        print("âŒ Analiz sonucu bulunamadÄ±!")

if __name__ == "__main__":
    main()
