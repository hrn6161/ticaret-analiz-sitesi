import requests
from bs4 import BeautifulSoup
import time
import random
import re
from openpyxl import Workbook
from openpyxl.styles import Font
import sys
import logging
import os
from datetime import datetime
import cloudscraper
from urllib.parse import urlparse
import concurrent.futures
import json

print("üöÄ KAPSAMLI Tƒ∞CARET ANALƒ∞Z Sƒ∞STEMƒ∞ BA≈ûLATILIYOR...")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'comprehensive_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

class AdvancedConfig:
    def __init__(self):
        self.MAX_RESULTS = 5
        self.REQUEST_TIMEOUT = 30
        self.RETRY_ATTEMPTS = 3
        self.MAX_GTIP_CHECK = 5
        
        self.USER_AGENTS = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
        ]
        
        self.SEARCH_ENGINES = ["google", "duckduckgo", "bing"]
        self.TRADE_SITES = ["trademo.com", "eximpedia.app", "volza.com", "importyet.com"]

class MultiSearchEngine:
    def __init__(self, config):
        self.config = config
        self.scraper = cloudscraper.create_scraper()
    
    def search_all_engines(self, query):
        """T√ºm arama motorlarƒ±nda ara"""
        all_results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_engine = {
                executor.submit(self._search_google, query): "google",
                executor.submit(self._search_duckduckgo, query): "duckduckgo", 
                executor.submit(self._search_bing, query): "bing"
            }
            
            for future in concurrent.futures.as_completed(future_to_engine):
                engine = future_to_engine[future]
                try:
                    results = future.result()
                    all_results.extend(results)
                    print(f"   ‚úÖ {engine}: {len(results)} sonu√ß")
                except Exception as e:
                    print(f"   ‚ùå {engine} hatasƒ±: {e}")
        
        return all_results
    
    def _search_google(self, query):
        """Google aramasƒ±"""
        try:
            url = "https://www.google.com/search"
            params = {"q": query, "num": self.config.MAX_RESULTS}
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            response = self.scraper.get(url, params=params, headers=headers, timeout=15)
            return self._parse_google_results(response.text)
        except Exception as e:
            print(f"   ‚ùå Google search error: {e}")
            return []
    
    def _parse_google_results(self, html):
        """Google sonu√ßlarƒ±nƒ± parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        for g in soup.find_all('div', class_='g'):
            try:
                title_elem = g.find('h3')
                if not title_elem:
                    continue
                
                title = title_elem.get_text()
                
                link_elem = g.find('a')
                url = link_elem.get('href') if link_elem else ""
                
                if url.startswith('/url?q='):
                    url = url.split('/url?q=')[1].split('&')[0]
                
                snippet_elem = g.find('div', class_='VwiC3b')
                snippet = snippet_elem.get_text() if snippet_elem else ""
                
                if url and url.startswith('http'):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet,
                        'domain': self._extract_domain(url),
                        'search_engine': 'google'
                    })
            except Exception as e:
                continue
        
        return results
    
    def _search_duckduckgo(self, query):
        """DuckDuckGo aramasƒ±"""
        try:
            url = "https://html.duckduckgo.com/html/"
            data = {'q': query, 'b': ''}
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Content-Type': 'application/x-www-form-urlencoded',
            }
            
            response = self.scraper.post(url, data=data, headers=headers, timeout=15)
            return self._parse_duckduckgo_results(response.text)
        except Exception as e:
            print(f"   ‚ùå DuckDuckGo search error: {e}")
            return []
    
    def _parse_duckduckgo_results(self, html):
        """DuckDuckGo sonu√ßlarƒ±nƒ± parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        for result in soup.find_all('div', class_='result'):
            try:
                title_elem = result.find('a', class_='result__a')
                if not title_elem:
                    continue
                
                title = title_elem.get_text()
                url = title_elem.get('href')
                
                if url and '//duckduckgo.com/l/' in url:
                    try:
                        redirect_response = self.scraper.get(url, timeout=5, allow_redirects=True)
                        url = redirect_response.url
                    except:
                        pass
                
                snippet_elem = result.find('a', class_='result__snippet')
                snippet = snippet_elem.get_text() if snippet_elem else ""
                
                if url and url.startswith('http'):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet,
                        'domain': self._extract_domain(url),
                        'search_engine': 'duckduckgo'
                    })
            except Exception as e:
                continue
        
        return results
    
    def _search_bing(self, query):
        """Bing aramasƒ±"""
        try:
            url = "https://www.bing.com/search"
            params = {"q": query, "count": self.config.MAX_RESULTS}
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
            }
            
            response = self.scraper.get(url, params=params, headers=headers, timeout=15)
            return self._parse_bing_results(response.text)
        except Exception as e:
            print(f"   ‚ùå Bing search error: {e}")
            return []
    
    def _parse_bing_results(self, html):
        """Bing sonu√ßlarƒ±nƒ± parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        for li in soup.find_all('li', class_='b_algo'):
            try:
                title_elem = li.find('h2')
                if not title_elem:
                    continue
                
                title = title_elem.get_text()
                
                link_elem = li.find('a')
                url = link_elem.get('href') if link_elem else ""
                
                snippet_elem = li.find('div', class_='b_caption')
                snippet = snippet_elem.get_text() if snippet_elem else ""
                
                if url and url.startswith('http'):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet,
                        'domain': self._extract_domain(url),
                        'search_engine': 'bing'
                    })
            except Exception as e:
                continue
        
        return results
    
    def _extract_domain(self, url):
        """URL'den domain √ßƒ±kar"""
        try:
            return urlparse(url).netloc
        except:
            return ""

class AdvancedCrawler:
    def __init__(self, config):
        self.config = config
        self.scraper = cloudscraper.create_scraper()
    
    def crawl_with_retry(self, url, target_country):
        """Retry mekanizmalƒ± crawl"""
        for attempt in range(self.config.RETRY_ATTEMPTS):
            try:
                result = self._smart_crawl(url, target_country)
                if result['status_code'] == 200:
                    return result
                
                print(f"   ‚è∞ Deneme {attempt + 1} ba≈üarƒ±sƒ±z, {2 ** attempt}s bekleniyor...")
                time.sleep(2 ** attempt)
                
            except Exception as e:
                print(f"   ‚ùå Crawl deneme {attempt + 1} hatasƒ±: {e}")
                time.sleep(2 ** attempt)
        
        return self._analyze_fallback(url, target_country)
    
    def _smart_crawl(self, url, target_country):
        """Akƒ±llƒ± crawl stratejisi"""
        domain = self._extract_domain(url)
        
        if any(trade_site in domain for trade_site in self.config.TRADE_SITES):
            return self._crawl_trade_site(url, target_country, domain)
        else:
            return self._crawl_general_site(url, target_country)
    
    def _crawl_trade_site(self, url, target_country, domain):
        """Ticaret siteleri i√ßin √∂zel crawl"""
        headers = self._get_advanced_headers(domain)
        
        try:
            response = self.scraper.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                return self._parse_trade_site_content(response.text, target_country, domain, response.status_code)
            elif response.status_code == 403:
                print(f"   üîí 403 Forbidden: {domain}")
                return self._analyze_trade_site_fallback(url, target_country, domain)
            else:
                return {'country_found': False, 'gtip_codes': [], 'status_code': response.status_code}
                
        except Exception as e:
            print(f"   ‚ùå Trade site crawl hatasƒ±: {e}")
            return self._analyze_trade_site_fallback(url, target_country, domain)
    
    def _crawl_general_site(self, url, target_country):
        """Genel siteler i√ßin crawl"""
        headers = self._get_advanced_headers()
        
        try:
            response = self.scraper.get(url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                return self._parse_general_content(response.text, target_country, response.status_code)
            else:
                return {'country_found': False, 'gtip_codes': [], 'status_code': response.status_code}
                
        except Exception as e:
            print(f"   ‚ùå General site crawl hatasƒ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'status_code': 'ERROR'}
    
    def _get_advanced_headers(self, domain=None):
        """Geli≈ümi≈ü headers"""
        headers = {
            'User-Agent': random.choice(self.config.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        if domain and 'trademo.com' in domain:
            headers.update({'Referer': 'https://www.trademo.com/'})
        elif domain and 'eximpedia.app' in domain:
            headers.update({'Referer': 'https://www.eximpedia.app/'})
        
        return headers
    
    def _parse_trade_site_content(self, html, target_country, domain, status_code):
        """Ticaret sitesi i√ßeriƒüini parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        text_content = soup.get_text()
        text_lower = text_content.lower()
        
        country_found = self._advanced_country_detection(text_lower, target_country, domain)
        gtip_codes = self._advanced_gtip_extraction(text_content, domain)
        
        print(f"   üîç {domain} analiz: √úlke={country_found}, GTIP={gtip_codes}")
        
        return {
            'country_found': country_found,
            'gtip_codes': gtip_codes,
            'content_preview': text_content[:500] + "..." if len(text_content) > 500 else text_content,
            'status_code': status_code
        }
    
    def _parse_general_content(self, html, target_country, status_code):
        """Genel site i√ßeriƒüini parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        text_content = soup.get_text()
        text_lower = text_content.lower()
        
        country_found = self._check_country_basic(text_lower, target_country)
        gtip_codes = self._extract_gtip_basic(text_content)
        
        return {
            'country_found': country_found,
            'gtip_codes': gtip_codes,
            'content_preview': text_content[:300] + "..." if len(text_content) > 300 else text_content,
            'status_code': status_code
        }
    
    def _advanced_country_detection(self, text_lower, target_country, domain):
        """Geli≈ümi≈ü √ºlke tespiti"""
        russia_patterns = ['russia', 'rusya', 'russian', 'rusian', 'rus']
        
        trade_context_patterns = [
            (r'export.*russia', 'export_russia'),
            (r'import.*russia', 'import_russia'),
            (r'destination.*russia', 'destination_russia'),
            (r'country of export.*russia', 'country_export_russia'),
        ]
        
        # Domain'e √∂zel pattern'ler
        if 'eximpedia.app' in domain:
            if any(pattern in text_lower for pattern in ['destination russia', 'export russia']):
                print("   ‚úÖ Eximpedia: Destination Russia tespit edildi")
                return True
                
        if 'trademo.com' in domain:
            if any(pattern in text_lower for pattern in ['country of export russia', 'export country russia']):
                print("   ‚úÖ Trademo: Country of Export Russia tespit edildi")
                return True
        
        # Genel pattern kontrol√º
        for pattern, context in trade_context_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                print(f"   ‚úÖ Trade context bulundu: {context}")
                return True
        
        for country_pattern in russia_patterns:
            if country_pattern in text_lower:
                trade_terms = ['export', 'import', 'trade', 'shipment']
                for term in trade_terms:
                    if term in text_lower:
                        print(f"   ‚úÖ √úlke+Trade baƒülantƒ±sƒ±: {country_pattern} + {term}")
                        return True
                return True
        
        return False
    
    def _advanced_gtip_extraction(self, text, domain):
        """Geli≈ümi≈ü GTIP √ßƒ±karma"""
        patterns = [
            r'\b\d{4}\.?\d{0,4}\b',
            r'\b\d{6}\b',
            r'\bHS\s?CODE\s?:?\s?(\d{4,8})\b',
            r'\bGTIP\s?:?\s?(\d{4,8})\b',
            r'\bH\.S\.\s?CODE?\s?:?\s?(\d{4,8})\b',
        ]
        
        codes = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                
                code = re.sub(r'[^\d]', '', match)
                if len(code) >= 4:
                    codes.add(code[:4])
                    print(f"   üîç GTIP bulundu: {code[:4]}")
        
        # Otomatik 8708 ekleme
        if any(site in domain for site in self.config.TRADE_SITES):
            if any(keyword in text.lower() for keyword in ['vehicle', 'automotive', 'motor', '8708', '870830']):
                codes.add('8708')
                print("   üîç Otomatik 8708 eklendi")
        
        return list(codes)
    
    def _check_country_basic(self, text_lower, target_country):
        """Temel √ºlke kontrol√º"""
        return any(pattern in text_lower for pattern in ['russia', 'rusya', 'russian'])
    
    def _extract_gtip_basic(self, text):
        """Temel GTIP √ßƒ±karma"""
        matches = re.findall(r'\b\d{4}\b', text)
        return list(set(matches))
    
    def _analyze_trade_site_fallback(self, url, target_country, domain):
        """Ticaret sitesi fallback analizi"""
        url_lower = url.lower()
        
        country_found = 'russia' in url_lower or 'rusya' in url_lower
        gtip_found = '8708' in url_lower
        
        return {
            'country_found': country_found,
            'gtip_codes': ['8708'] if gtip_found else [],
            'status_code': 'URL_ANALYSIS'
        }
    
    def _analyze_fallback(self, url, target_country):
        """Genel fallback analizi"""
        return {
            'country_found': False,
            'gtip_codes': [],
            'status_code': 'FALLBACK'
        }
    
    def _extract_domain(self, url):
        """URL'den domain √ßƒ±kar"""
        try:
            return urlparse(url).netloc
        except:
            return ""

class SanctionChecker:
    def __init__(self, config):
        self.config = config
        self.sanctioned_codes = {
            '8708': 'Motorlu ta≈üƒ±tlar yedek par√ßalarƒ±',
            '8711': 'Motorsikletler',
            '8703': 'Motorlu ta≈üƒ±tlar',
            '8408': 'Dizel motorlar',
        }
    
    def check_sanctions(self, gtip_codes):
        """Yaptƒ±rƒ±m kontrol√º"""
        sanctioned = []
        reasons = []
        
        for code in gtip_codes[:self.config.MAX_GTIP_CHECK]:
            if code in self.sanctioned_codes:
                sanctioned.append(code)
                reasons.append(f"{code}: {self.sanctioned_codes[code]}")
        
        return sanctioned, reasons

class ComprehensiveTradeAnalyzer:
    def __init__(self, config):
        self.config = config
        self.searcher = MultiSearchEngine(config)
        self.crawler = AdvancedCrawler(config)
        self.sanction_checker = SanctionChecker(config)
    
    def comprehensive_analyze(self, company, country):
        """Kapsamlƒ± analiz"""
        print(f"üöÄ KAPSAMLI ANALƒ∞Z: {company} ‚Üî {country}")
        
        queries = self._generate_comprehensive_queries(company, country)
        all_results = []
        
        # Paralel arama
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            search_futures = [executor.submit(self.searcher.search_all_engines, query) for query in queries]
            
            for future in concurrent.futures.as_completed(search_futures):
                try:
                    search_results = future.result()
                    for result in search_results:
                        if self._is_relevant_result(result, company, country):
                            analysis = self._analyze_single_result(result, company, country)
                            if analysis:
                                all_results.append(analysis)
                except Exception as e:
                    print(f"   ‚ùå Arama hatasƒ±: {e}")
        
        unique_results = self._remove_duplicates(all_results)
        return unique_results
    
    def _generate_comprehensive_queries(self, company, country):
        """Kapsamlƒ± sorgular olu≈ütur"""
        base_queries = [
            f'"{company}" "{country}" export',
            f'"{company}" "{country}" import',
            f'"{company}" "{country}" trade',
            f'"{company}" "{country}" shipment',
            f'"{company}" "{country}" supplier',
            f'"{company}" "{country}" HS code',
            f'"{company}" "{country}" GTIP',
        ]
        
        site_specific_queries = []
        for site in self.config.TRADE_SITES:
            site_specific_queries.extend([
                f'site:{site} "{company}"',
                f'site:{site} "{company}" {country}',
            ])
        
        return base_queries + site_specific_queries
    
    def _is_relevant_result(self, result, company, country):
        """Sonucun alakalƒ± olup olmadƒ±ƒüƒ±nƒ± kontrol et"""
        domain = result.get('domain', '').lower()
        title = result.get('title', '').lower()
        snippet = result.get('snippet', '').lower()
        
        combined_text = f"{title} {snippet}"
        
        if any(trade_site in domain for trade_site in self.config.TRADE_SITES):
            return True
        
        if any(keyword in combined_text for keyword in [country.lower(), 'russia', 'rusya', '8708', 'hs code', 'gtip']):
            return True
        
        return False
    
    def _analyze_single_result(self, result, company, country):
        """Tekil sonucu analiz et"""
        try:
            crawl_result = self.crawler.crawl_with_retry(result['url'], country)
            
            sanctioned_gtips, sanction_reasons = self.sanction_checker.check_sanctions(crawl_result['gtip_codes'])
            
            confidence = self._calculate_confidence(crawl_result, sanctioned_gtips, result['domain'])
            
            analysis = self._create_comprehensive_analysis(
                company, country, result, crawl_result, sanctioned_gtips, sanction_reasons, confidence
            )
            
            return analysis
            
        except Exception as e:
            print(f"   ‚ùå Sonu√ß analiz hatasƒ±: {e}")
            return None
    
    def _calculate_confidence(self, crawl_result, sanctioned_gtips, domain):
        """G√ºven seviyesi hesapla"""
        confidence = 0
        
        if any(trade_site in domain for trade_site in self.config.TRADE_SITES):
            confidence += 40
        
        if crawl_result['country_found']:
            confidence += 30
        
        if crawl_result['gtip_codes']:
            confidence += 20
        
        if sanctioned_gtips:
            confidence += 10
        
        return min(confidence, 100)
    
    def _create_comprehensive_analysis(self, company, country, search_result, crawl_result, sanctioned_gtips, sanction_reasons, confidence):
        """Kapsamlƒ± analiz sonucu olu≈ütur"""
        
        if sanctioned_gtips:
            status = "Y√úKSEK_RISK"
            explanation = f"‚õî Y√úKSEK Rƒ∞SK: {company} ≈üirketi {country} ile yaptƒ±rƒ±mlƒ± √ºr√ºn ticareti yapƒ±yor"
            advice = f"‚õî ACƒ∞L DURUM! Yaptƒ±rƒ±mlƒ± GTIP kodlarƒ±: {', '.join(sanctioned_gtips)}"
            risk_level = "Y√úKSEK"
        elif crawl_result['country_found'] and crawl_result['gtip_codes']:
            status = "ORTA_RISK"
            explanation = f"üü° ORTA Rƒ∞SK: {company} ≈üirketinin {country} ile ticaret baƒülantƒ±sƒ± bulundu"
            advice = f"Ticaret baƒülantƒ±sƒ± doƒürulandƒ±. GTIP: {', '.join(crawl_result['gtip_codes'][:3])}"
            risk_level = "ORTA"
        elif crawl_result['country_found']:
            status = "D√ú≈û√úK_RISK"
            explanation = f"üü¢ D√ú≈û√úK Rƒ∞SK: {company} ≈üirketinin {country} ile baƒülantƒ±sƒ± var"
            advice = "Ticaret baƒülantƒ±sƒ± bulundu ancak GTIP kodu tespit edilemedi"
            risk_level = "D√ú≈û√úK"
        else:
            status = "TEMIZ"
            explanation = f"‚úÖ TEMƒ∞Z: {company} ≈üirketinin {country} ile ticaret baƒülantƒ±sƒ± bulunamadƒ±"
            advice = "Risk tespit edilmedi"
            risk_level = "YOK"
        
        return {
            '≈ûƒ∞RKET': company,
            '√úLKE': country,
            'DURUM': status,
            'AI_A√áIKLAMA': explanation,
            'AI_TAVSIYE': advice,
            'YAPTIRIM_RISKI': risk_level,
            'TESPIT_EDILEN_GTIPLER': ', '.join(crawl_result['gtip_codes']),
            'YAPTIRIMLI_GTIPLER': ', '.join(sanctioned_gtips),
            'YAPTIRIM_NEDENLERI': ' | '.join(sanction_reasons),
            'ULKE_BAGLANTISI': 'EVET' if crawl_result['country_found'] else 'HAYIR',
            'BA≈ûLIK': search_result['title'],
            'URL': search_result['url'],
            '√ñZET': search_result['snippet'],
            'STATUS_CODE': crawl_result.get('status_code', 'N/A'),
            'KAYNAK_DOMAIN': search_result['domain'],
            'ARAMA_MOTORU': search_result.get('search_engine', 'N/A'),
            'G√úVEN_SEVƒ∞YESƒ∞': f"%{confidence}",
            'ƒ∞√áERƒ∞K_√ñNƒ∞ZLEME': crawl_result.get('content_preview', '')[:200] + "..." if crawl_result.get('content_preview') else ''
        }
    
    def _remove_duplicates(self, results):
        """Benzersiz sonu√ßlar"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('URL', '')
            if url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        return unique_results

def create_comprehensive_excel_report(results, company, country):
    """Kapsamlƒ± Excel raporu"""
    try:
        filename = f"{company.replace(' ', '_')}_{country}_kapsamlƒ±_analiz.xlsx"
        
        wb = Workbook()
        
        # 1. Sayfa: Detaylƒ± Sonu√ßlar
        ws1 = wb.active
        ws1.title = "Detaylƒ± Analiz"
        
        headers = [
            '≈ûƒ∞RKET', '√úLKE', 'DURUM', 'YAPTIRIM_RISKI', 'ULKE_BAGLANTISI',
            'TESPIT_EDILEN_GTIPLER', 'YAPTIRIMLI_GTIPLER', 'YAPTIRIM_NEDENLERI',
            'G√úVEN_SEVƒ∞YESƒ∞', 'KAYNAK_DOMAIN', 'ARAMA_MOTORU', 'STATUS_CODE',
            'AI_A√áIKLAMA', 'AI_TAVSIYE', 'BA≈ûLIK', 'URL', '√ñZET', 'ƒ∞√áERƒ∞K_√ñNƒ∞ZLEME'
        ]
        
        for col, header in enumerate(headers, 1):
            cell = ws1.cell(row=1, column=col, value=header)
            cell.font = Font(bold=True)
        
        for row, result in enumerate(results, 2):
            for col, key in enumerate(headers, 1):
                ws1.cell(row=row, column=col, value=str(result.get(key, '')))
        
        # 2. Sayfa: √ñzet
        ws2 = wb.create_sheet("Analiz √ñzeti")
        
        summary_data = [
            ["≈ûirket", company],
            ["√úlke", country],
            ["Analiz Tarihi", datetime.now().strftime("%d/%m/%Y %H:%M")],
            ["Toplam Sonu√ß", len(results)],
            ["Y√ºksek Risk", len([r for r in results if r.get('YAPTIRIM_RISKI') == 'Y√úKSEK'])],
            ["Orta Risk", len([r for r in results if r.get('YAPTIRIM_RISKI') == 'ORTA'])],
            ["D√º≈ü√ºk Risk", len([r for r in results if r.get('YAPTIRIM_RISKI') == 'D√ú≈û√úK'])],
            ["Temiz", len([r for r in results if r.get('YAPTIRIM_RISKI') == 'YOK'])],
            ["√úlke Baƒülantƒ±sƒ±", len([r for r in results if r.get('ULKE_BAGLANTISI') == 'EVET'])],
        ]
        
        for i, (label, value) in enumerate(summary_data, 1):
            ws2.cell(row=i, column=1, value=label).font = Font(bold=True)
            ws2.cell(row=i, column=2, value=value)
        
        # Stil ayarlarƒ±
        for column in ws1.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws1.column_dimensions[column_letter].width = adjusted_width
        
        ws2.column_dimensions['A'].width = 20
        ws2.column_dimensions['B'].width = 30
        
        wb.save(filename)
        print(f"‚úÖ Kapsamlƒ± Excel raporu olu≈üturuldu: {filename}")
        return filename
        
    except Exception as e:
        print(f"‚ùå Excel rapor hatasƒ±: {e}")
        return None

def display_comprehensive_results(results, company, country):
    """Sonu√ßlarƒ± detaylƒ± g√∂ster"""
    print(f"\n{'='*100}")
    print(f"üìä KAPSAMLI ANALƒ∞Z SONU√áLARI: {company} ‚Üî {country}")
    print(f"{'='*100}")
    
    if not results:
        print("‚ùå Hi√ßbir sonu√ß bulunamadƒ±!")
        return
    
    # √ñzet istatistikler
    high_risk = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'Y√úKSEK'])
    medium_risk = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'ORTA'])
    low_risk = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'D√ú≈û√úK'])
    clean = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'YOK'])
    country_connections = len([r for r in results if r.get('ULKE_BAGLANTISI') == 'EVET'])
    
    print(f"\nüìà √ñZET ƒ∞STATƒ∞STƒ∞KLER:")
    print(f"   ‚Ä¢ Toplam Sonu√ß: {len(results)}")
    print(f"   ‚Ä¢ Y√úKSEK Risk: {high_risk}")
    print(f"   ‚Ä¢ ORTA Risk: {medium_risk}")
    print(f"   ‚Ä¢ D√ú≈û√úK Risk: {low_risk}")
    print(f"   ‚Ä¢ TEMƒ∞Z: {clean}")
    print(f"   ‚Ä¢ √úlke Baƒülantƒ±sƒ±: {country_connections}")
    
    if high_risk > 0:
        print(f"\n‚ö†Ô∏è  KRƒ∞Tƒ∞K YAPTIRIM UYARILARI:")
        for result in results:
            if result.get('YAPTIRIM_RISKI') == 'Y√úKSEK':
                print(f"   üî¥ {result.get('BA≈ûLIK', '')[:70]}...")
                print(f"      üö´ Yaptƒ±rƒ±mlƒ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
                print(f"      üìä G√ºven: {result.get('G√úVEN_SEVƒ∞YESƒ∞', '')}")
                print(f"      üåê Kaynak: {result.get('KAYNAK_DOMAIN', '')}")
    
    # Detaylƒ± sonu√ßlar
    for i, result in enumerate(results, 1):
        risk_color = "üî¥" if result.get('YAPTIRIM_RISKI') == 'Y√úKSEK' else "üü°" if result.get('YAPTIRIM_RISKI') == 'ORTA' else "üü¢" if result.get('YAPTIRIM_RISKI') == 'D√ú≈û√úK' else "‚úÖ"
        
        print(f"\n{risk_color} SONU√á {i}:")
        print(f"   üìù Ba≈ülƒ±k: {result.get('BA≈ûLIK', 'N/A')}")
        print(f"   üåê URL: {result.get('URL', 'N/A')}")
        print(f"   üéØ Durum: {result.get('DURUM', 'N/A')}")
        print(f"   ‚ö†Ô∏è  Risk Seviyesi: {result.get('YAPTIRIM_RISKI', 'N/A')}")
        print(f"   üîó √úlke Baƒülantƒ±sƒ±: {result.get('ULKE_BAGLANTISI', 'N/A')}")
        print(f"   üîç GTIP Kodlarƒ±: {result.get('TESPIT_EDILEN_GTIPLER', 'Yok')}")
        
        if result.get('YAPTIRIMLI_GTIPLER'):
            print(f"   üö´ Yaptƒ±rƒ±mlƒ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
            print(f"   üìã Yaptƒ±rƒ±m Nedenleri: {result.get('YAPTIRIM_NEDENLERI', '')}")
        
        print(f"   üìä G√ºven Seviyesi: {result.get('G√úVEN_SEVƒ∞YESƒ∞', 'N/A')}")
        print(f"   üåê Kaynak Domain: {result.get('KAYNAK_DOMAIN', 'N/A')}")
        print(f"   üîç Arama Motoru: {result.get('ARAMA_MOTORU', 'N/A')}")
        print(f"   üìã √ñzet: {result.get('√ñZET', 'N/A')[:100]}...")
        print(f"   üí° AI A√ßƒ±klama: {result.get('AI_A√áIKLAMA', 'N/A')}")
        print(f"   üí≠ AI Tavsiye: {result.get('AI_TAVSIYE', 'N/A')}")
        print(f"   {'‚îÄ'*80}")

def main():
    print("üöÄ KAPSAMLI Tƒ∞CARET ANALƒ∞Z Sƒ∞STEMƒ∞")
    print("üéØ √ñZELLƒ∞KLER: √áoklu Arama Motoru + Paralel ƒ∞≈ülem + Geli≈ümi≈ü Pattern Matching")
    print("üí° HEDEF: Eximpedia Destination Russia ve Trademo Country of Export tespiti")
    print("üìä RAPOR: Detaylƒ± Excel raporu ve kapsamlƒ± analiz\n")
    
    config = AdvancedConfig()
    analyzer = ComprehensiveTradeAnalyzer(config)
    
    company = input("≈ûirket adƒ±nƒ± girin: ").strip()
    country = input("√úlke adƒ±nƒ± girin: ").strip()
    
    if not company or not country:
        print("‚ùå ≈ûirket ve √ºlke bilgisi gereklidir!")
        return
    
    print(f"\nüîç KAPSAMLI ANALƒ∞Z BA≈ûLATILIYOR...")
    print("   ‚ö° 3 arama motoru paralel √ßalƒ±≈üƒ±yor...")
    print("   üîÑ Retry mekanizmasƒ± aktif...")
    print("   üéØ Geli≈ümi≈ü pattern matching uygulanƒ±yor...")
    print("   üìä Excel raporu hazƒ±rlanƒ±yor...\n")
    
    start_time = time.time()
    
    results = analyzer.comprehensive_analyze(company, country)
    
    execution_time = time.time() - start_time
    
    # Sonu√ßlarƒ± g√∂ster
    display_comprehensive_results(results, company, country)
    
    # Excel raporu olu≈ütur
    excel_file = create_comprehensive_excel_report(results, company, country)
    
    print(f"\n‚è±Ô∏è  Toplam √ßalƒ±≈üma s√ºresi: {execution_time:.2f} saniye")
    print(f"üìä Toplam sonu√ß: {len(results)}")
    
    if excel_file:
        print(f"üìÅ Excel raporu: {excel_file}")
        
        # Excel a√ßma se√ßeneƒüi
        try:
            open_excel = input("\nüìÇ Excel dosyasƒ±nƒ± a√ßmak ister misiniz? (e/h): ").strip().lower()
            if open_excel == 'e':
                if os.name == 'nt':
                    os.system(f'start excel "{excel_file}"')
                elif os.name == 'posix':
                    os.system(f'open "{excel_file}"' if sys.platform == 'darwin' else f'xdg-open "{excel_file}"')
                print("üìÇ Excel dosyasƒ± a√ßƒ±lƒ±yor...")
        except Exception as e:
            print(f"‚ö†Ô∏è  Dosya otomatik a√ßƒ±lamadƒ±: {e}")

if __name__ == "__main__":
    main()
