import requests
from bs4 import BeautifulSoup
import time
import random
import re
from openpyxl import Workbook
from openpyxl.styles import Font
import sys
import logging
import os
from datetime import datetime
import cloudscraper
import json

print("ğŸš€ DUCKDUCKGO ANA, GOOGLE FALLBACK Ä°LE TÄ°CARET ANALÄ°Z SÄ°STEMÄ°")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

class Config:
    def __init__(self):
        self.MAX_RESULTS = 5
        self.REQUEST_TIMEOUT = 30
        self.RETRY_ATTEMPTS = 2
        self.MAX_GTIP_CHECK = 3
        self.USER_AGENTS = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

class AdvancedCrawler:
    def __init__(self, config):
        self.config = config
        self.scraper = cloudscraper.create_scraper()
    
    def advanced_crawl(self, url, target_country):
        """GeliÅŸmiÅŸ crawl"""
        print(f"   ğŸŒ Crawl: {url[:60]}...")
        
        domain = self._extract_domain(url)
        if any(site in domain for site in ['trademo.com', 'volza.com', 'eximpedia.app']):
            wait_time = random.uniform(8, 12)
            print(f"   â³ {domain} iÃ§in {wait_time:.1f}s bekleme...")
            time.sleep(wait_time)
        
        page_result = self._try_cloudscraper_crawl(url, target_country)
        if page_result['status_code'] == 200:
            return page_result
        
        page_result = self._try_page_crawl(url, target_country)
        if page_result['status_code'] == 200:
            return page_result
        
        print(f"   ğŸ” Snippet analizi: {url}")
        return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'SNIPPET_ANALYSIS'}
    
    def _try_cloudscraper_crawl(self, url, target_country):
        """Cloudscraper ile crawl"""
        try:
            print(f"   â˜ï¸ Cloudscraper: {url}")
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
            }
            
            response = self.scraper.get(url, headers=headers, timeout=25)
            
            if response.status_code == 200:
                print(f"   âœ… Cloudscraper baÅŸarÄ±lÄ±: {url}")
                return self._parse_advanced_content(response.text, target_country, response.status_code)
            else:
                print(f"   âŒ Cloudscraper hatasÄ± {response.status_code}: {url}")
                return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': response.status_code}
        except Exception as e:
            print(f"   âŒ Cloudscraper hatasÄ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'CLOUDSCRAPER_ERROR'}
    
    def _try_page_crawl(self, url, target_country):
        """Normal requests ile crawl"""
        try:
            print(f"   ğŸŒ Normal requests: {url}")
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
            }
            
            response = requests.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                print(f"   âœ… Normal requests baÅŸarÄ±lÄ±: {url}")
                return self._parse_advanced_content(response.text, target_country, response.status_code)
            elif response.status_code == 403:
                print(f"   ğŸ”’ 403 Forbidden: {url}")
                return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 403}
            else:
                print(f"   âŒ Sayfa hatasÄ± {response.status_code}: {url}")
                return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': response.status_code}
        except Exception as e:
            print(f"   âŒ Sayfa crawl hatasÄ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'ERROR'}
    
    def _parse_advanced_content(self, html, target_country, status_code):
        """Ä°Ã§erik analizi"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            text_content = soup.get_text()
            text_lower = text_content.lower()
            
            country_found = self._check_country_advanced(text_lower, target_country)
            gtip_codes = self.extract_advanced_gtip_codes(text_content)
            
            print(f"   ğŸ” Sayfa analizi: Ãœlke={country_found}, GTIP={gtip_codes[:3]}")
            
            return {
                'country_found': country_found,
                'gtip_codes': gtip_codes,
                'content_preview': text_content[:200] + "..." if len(text_content) > 200 else text_content,
                'status_code': status_code
            }
        except Exception as e:
            print(f"   âŒ Parse hatasÄ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'PARSE_ERROR'}
    
    def _check_country_advanced(self, text_lower, target_country):
        """Ãœlke kontrolÃ¼"""
        country_variations = [
            target_country.lower(),
            target_country.upper(), 
            target_country.title(),
            'russia', 'rusya', 'rusian', 'rus'
        ]
        
        trade_terms = ['export', 'import', 'country', 'origin', 'destination', 'trade']
        
        for country_var in country_variations:
            if country_var in text_lower:
                for term in trade_terms:
                    if f"{term} {country_var}" in text_lower or f"{country_var} {term}" in text_lower:
                        return True
                return True
        
        return False
    
    def extract_advanced_gtip_codes(self, text):
        """GTIP kod Ã§Ä±karma"""
        patterns = [
            r'\b\d{4}\.?\d{0,4}\b',
            r'\b\d{6}\b',
            r'\bHS\s?CODE\s?:?\s?(\d{4,8})\b',
            r'\bGTIP\s?:?\s?(\d{4,8})\b',
        ]
        
        all_codes = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                
                code = re.sub(r'[^\d]', '', match)
                if len(code) >= 4:
                    all_codes.add(code[:4])
        
        return list(all_codes)
    
    def analyze_snippet_deep(self, snippet_text, target_country):
        """Snippet analizi"""
        if not snippet_text:
            return {'country_found': False, 'gtip_codes': []}
        
        text_lower = snippet_text.lower()
        
        country_found = self._check_country_advanced(text_lower, target_country)
        gtip_codes = self.extract_advanced_gtip_codes(snippet_text)
        
        return {
            'country_found': country_found,
            'gtip_codes': gtip_codes
        }
    
    def _extract_domain(self, url):
        """URL'den domain Ã§Ä±kar"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""

class GoogleSearcher:
    def __init__(self, config):
        self.config = config
        # GOOGLE API CREDENTIALS
        self.google_api_key = "AIzaSyC2A3ANshAolgr4hNNlFOtgNSlcQtIP40Y"
        self.google_cse_id = "d65dec7934a544da1"
        
        print(f"   ğŸ”‘ Google API Key: {self.google_api_key[:10]}...")
        print(f"   ğŸ” Google CSE ID: {self.google_cse_id}")
        print("   âœ… Google Custom Search API hazÄ±r!")
    
    def google_search(self, query, max_results=5):
        """Google Custom Search API ile arama - GeliÅŸmiÅŸ hata yÃ¶netimi"""
        try:
            print(f"   ğŸ” Google Search: {query}")
            
            wait_time = random.uniform(1, 3)
            time.sleep(wait_time)
            
            endpoint = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': self.google_api_key,
                'cx': self.google_cse_id,
                'q': query,
                'num': min(max_results, 10)
            }
            
            print(f"   ğŸŒ Google API isteÄŸi: {query}")
            response = requests.get(endpoint, params=params, timeout=10)
            
            # Response tipini kontrol et
            content_type = response.headers.get('content-type', '')
            
            if 'application/json' not in content_type:
                print(f"   âŒ Google API JSON yerine HTML dÃ¶ndÃ¼rdÃ¼: {content_type}")
                print(f"   âŒ Response ilk 200 karakter: {response.text[:200]}")
                return []
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    results = self.parse_google_results(data)
                    print(f"   âœ… Google {len(results)} sonuÃ§ buldu")
                    return results
                except json.JSONDecodeError as e:
                    print(f"   âŒ JSON decode hatasÄ±: {e}")
                    print(f"   âŒ Response: {response.text[:500]}")
                    return []
            else:
                print(f"   âŒ Google API hatasÄ± {response.status_code}: {response.text[:200]}")
                return []
                
        except requests.exceptions.RequestException as e:
            print(f"   âŒ Google API baÄŸlantÄ± hatasÄ±: {e}")
            return []
        except Exception as e:
            print(f"   âŒ Google arama hatasÄ±: {e}")
            return []
    
    def parse_google_results(self, data):
        """Google sonuÃ§larÄ±nÄ± parse et"""
        results = []
        
        if 'items' in data:
            for item in data['items']:
                results.append({
                    'title': item.get('title', ''),
                    'url': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'full_text': f"{item.get('title', '')} {item.get('snippet', '')}",
                    'domain': self._extract_domain(item.get('link', '')),
                    'search_engine': 'google'
                })
                
                print(f"      ğŸ“„ Google: {item.get('title', '')[:50]}...")
                print(f"      ğŸŒ URL: {item.get('link', '')[:80]}...")
        
        return results
    
    def _extract_domain(self, url):
        """URL'den domain Ã§Ä±kar"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""

class DuckDuckGoSearcher:
    def __init__(self, config):
        self.config = config
    
    def duckduckgo_search(self, query, max_results=5):
        """DuckDuckGo arama - Ana yÃ¶ntem olarak"""
        try:
            print(f"   ğŸ” DuckDuckGo Search: {query}")
            
            wait_time = random.uniform(3, 5)
            time.sleep(wait_time)
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            }
            
            url = "https://html.duckduckgo.com/html/"
            data = {'q': query, 'b': '', 'kl': 'us-en'}
            
            scraper = cloudscraper.create_scraper()
            response = scraper.post(url, data=data, headers=headers, timeout=15)
            
            if response.status_code == 200:
                results = self.parse_duckduckgo_results(response.text, max_results)
                print(f"   âœ… DuckDuckGo {len(results)} sonuÃ§ buldu")
                return results
            else:
                print(f"   âŒ DuckDuckGo hatasÄ± {response.status_code}")
                return []
        except Exception as e:
            print(f"   âŒ DuckDuckGo arama hatasÄ±: {e}")
            return []
    
    def parse_duckduckgo_results(self, html, max_results):
        """DuckDuckGo sonuÃ§larÄ±nÄ± parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        for div in soup.find_all('div', class_='result')[:max_results]:
            try:
                title_elem = div.find('a', class_='result__a')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                url = title_elem.get('href')
                
                # Redirect handling
                if url and '//duckduckgo.com/l/' in url:
                    try:
                        time.sleep(1)
                        scraper = cloudscraper.create_scraper()
                        redirect_response = scraper.get(url, timeout=8, allow_redirects=True)
                        url = redirect_response.url
                    except:
                        pass
                
                snippet_elem = div.find('a', class_='result__snippet')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                if url and url.startswith('//'):
                    url = 'https:' + url
                
                results.append({
                    'title': title,
                    'url': url,
                    'snippet': snippet,
                    'full_text': f"{title} {snippet}",
                    'domain': self._extract_domain(url),
                    'search_engine': 'duckduckgo'
                })
                
                print(f"      ğŸ“„ DuckDuckGo: {title[:50]}...")
                
            except Exception as e:
                print(f"      âŒ DuckDuckGo sonuÃ§ parse hatasÄ±: {e}")
                continue
        
        return results
    
    def _extract_domain(self, url):
        """URL'den domain Ã§Ä±kar"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""

class EnhancedSearcher:
    def __init__(self, config):
        self.config = config
        self.google_searcher = GoogleSearcher(config)
        self.ddg_searcher = DuckDuckGoSearcher(config)
    
    def enhanced_search(self, query, max_results=5):
        """AkÄ±llÄ± arama - DuckDuckGo ana, Google fallback"""
        
        # Ã–nce DuckDuckGo ile dene (daha gÃ¼venilir)
        ddg_results = self.ddg_searcher.duckduckgo_search(query, max_results)
        if ddg_results:
            return ddg_results
        
        # DuckDuckGo baÅŸarÄ±sÄ±zsa Google fallback
        print("   ğŸ”„ DuckDuckGo sonuÃ§ vermedi, Google deneniyor...")
        google_results = self.google_searcher.google_search(query, max_results)
        return google_results

class QuickEURLexChecker:
    def __init__(self, config):
        self.config = config
        self.sanction_cache = {}
    
    def quick_check_gtip(self, gtip_codes):
        """HÄ±zlÄ± GTIP kontrolÃ¼"""
        if not gtip_codes:
            return []
            
        sanctioned_codes = []
        checked_codes = gtip_codes[:self.config.MAX_GTIP_CHECK]
        
        print(f"   ğŸ” EUR-Lex kontrolÃ¼: {checked_codes}")
        
        for gtip_code in checked_codes:
            if gtip_code in self.sanction_cache:
                if self.sanction_cache[gtip_code]:
                    sanctioned_codes.append(gtip_code)
                continue
                
            try:
                wait_time = random.uniform(2, 4)
                time.sleep(wait_time)
                
                url = "https://eur-lex.europa.eu/search.html"
                params = {
                    'text': f'"{gtip_code}" sanction prohibited',
                    'type': 'advanced',
                    'lang': 'en'
                }
                
                headers = {
                    'User-Agent': random.choice(self.config.USER_AGENTS),
                }
                
                response = requests.get(url, params=params, headers=headers, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content = soup.get_text().lower()
                    
                    sanction_terms = ['prohibited', 'banned', 'sanction', 'restricted']
                    found_sanction = any(term in content for term in sanction_terms)
                    
                    if found_sanction:
                        sanctioned_codes.append(gtip_code)
                        self.sanction_cache[gtip_code] = True
                        print(f"   â›” YaptÄ±rÄ±mlÄ± kod: {gtip_code}")
                    else:
                        self.sanction_cache[gtip_code] = False
                
            except Exception as e:
                print(f"   âŒ EUR-Lex kontrol hatasÄ±: {e}")
                continue
        
        return sanctioned_codes

class EnhancedTradeAnalyzer:
    def __init__(self, config):
        self.config = config
        self.searcher = EnhancedSearcher(config)
        self.crawler = AdvancedCrawler(config)
        self.eur_lex_checker = QuickEURLexChecker(config)
    
    def enhanced_analyze(self, company, country):
        """GeliÅŸmiÅŸ analiz - DuckDuckGo ana, Google fallback"""
        print(f"ğŸ¤– DUCKDUCKGO ANA, GOOGLE FALLBACK Ä°LE ANALÄ°Z BAÅLATILIYOR: {company} â†” {country}")
        
        search_queries = [
            f"{company} {country} trade",
            f"{company} {country} export", 
            f"{company} trade data",
            f"{company} {country} business",
            f"{company} {country} import export",
            f'"{company}" "{country}"',
            f"{company} {country} customs data"
        ]
        
        all_results = []
        
        for i, query in enumerate(search_queries, 1):
            try:
                print(f"\nğŸ” Sorgu {i}/{len(search_queries)}: {query}")
                
                if i > 1:
                    wait_time = random.uniform(8, 12)
                    print(f"   â³ Sorgular arasÄ± {wait_time:.1f}s bekleme...")
                    time.sleep(wait_time)
                
                search_results = self.searcher.enhanced_search(query, self.config.MAX_RESULTS)
                
                if not search_results:
                    print(f"   âš ï¸ SonuÃ§ bulunamadÄ±")
                    continue
                
                for j, result in enumerate(search_results, 1):
                    print(f"   ğŸ“„ SonuÃ§ {j}: {result['title'][:40]}...")
                    
                    if j > 1:
                        wait_time = random.uniform(4, 6)
                        time.sleep(wait_time)
                    
                    crawl_result = self.crawler.advanced_crawl(result['url'], country)
                    
                    if crawl_result['status_code'] != 200:
                        snippet_analysis = self.crawler.analyze_snippet_deep(result['full_text'], country)
                        if snippet_analysis['country_found'] or snippet_analysis['gtip_codes']:
                            crawl_result['country_found'] = snippet_analysis['country_found']
                            crawl_result['gtip_codes'] = snippet_analysis['gtip_codes']
                    
                    sanctioned_gtips = []
                    if crawl_result['gtip_codes']:
                        sanctioned_gtips = self.eur_lex_checker.quick_check_gtip(crawl_result['gtip_codes'])
                    
                    confidence = self._calculate_confidence(crawl_result, sanctioned_gtips, result['domain'])
                    
                    analysis = self.create_enhanced_analysis_result(
                        company, country, result, crawl_result, sanctioned_gtips, confidence
                    )
                    
                    all_results.append(analysis)
                
            except Exception as e:
                print(f"   âŒ Sorgu hatasÄ±: {e}")
                continue
        
        return all_results
    
    def _calculate_confidence(self, crawl_result, sanctioned_gtips, domain):
        """GÃ¼ven seviyesi hesapla"""
        confidence = 0
        
        trusted_domains = ['trademo.com', 'eximpedia.app', 'volza.com', 'importyet.com', 'emis.com']
        if any(trusted in domain for trusted in trusted_domains):
            confidence += 30
        
        if crawl_result['gtip_codes']:
            confidence += 25
        
        if crawl_result['country_found']:
            confidence += 25
        
        if sanctioned_gtips:
            confidence += 20
        
        return min(confidence, 100)
    
    def create_enhanced_analysis_result(self, company, country, search_result, crawl_result, sanctioned_gtips, confidence):
        """GeliÅŸmiÅŸ analiz sonucu"""
        
        reasons = []
        if crawl_result['country_found']:
            reasons.append("Ãœlke baÄŸlantÄ±sÄ± tespit edildi")
        if crawl_result['gtip_codes']:
            reasons.append(f"GTIP kodlarÄ± bulundu: {', '.join(crawl_result['gtip_codes'][:3])}")
        if sanctioned_gtips:
            reasons.append(f"YaptÄ±rÄ±mlÄ± GTIP kodlarÄ±: {', '.join(sanctioned_gtips)}")
        if any(trusted in search_result['domain'] for trusted in ['trademo.com', 'volza.com', 'eximpedia.app']):
            reasons.append("GÃ¼venilir ticaret verisi kaynaÄŸÄ±")
        
        if sanctioned_gtips:
            status = "YAPTIRIMLI_YÃœKSEK_RISK"
            explanation = f"â›” YÃœKSEK RÄ°SK: {company} ÅŸirketi {country} ile yaptÄ±rÄ±mlÄ± Ã¼rÃ¼n ticareti yapÄ±yor"
            ai_tavsiye = f"â›” ACÄ°L DURUM! Bu Ã¼rÃ¼nlerin {country.upper()} ihracÄ± YASAKTIR: {', '.join(sanctioned_gtips)}"
            risk_level = "YÃœKSEK"
        elif crawl_result['country_found'] and crawl_result['gtip_codes']:
            status = "RISK_VAR"
            explanation = f"ğŸŸ¡ RÄ°SK VAR: {company} ÅŸirketi {country} ile ticaret baÄŸlantÄ±sÄ± bulundu"
            ai_tavsiye = f"Ticaret baÄŸlantÄ±sÄ± doÄŸrulandÄ±. GTIP kodlarÄ±: {', '.join(crawl_result['gtip_codes'][:3])}"
            risk_level = "ORTA"
        elif crawl_result['country_found']:
            status = "Ä°LÄ°ÅKÄ°_VAR"
            explanation = f"ğŸŸ¢ Ä°LÄ°ÅKÄ° VAR: {company} ÅŸirketi {country} ile baÄŸlantÄ±lÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulundu ancak GTIP kodu tespit edilemedi"
            risk_level = "DÃœÅÃœK"
        else:
            status = "TEMIZ"
            explanation = f"âœ… TEMÄ°Z: {company} ÅŸirketinin {country} ile ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            risk_level = "YOK"
        
        return {
            'ÅÄ°RKET': company,
            'ÃœLKE': country,
            'DURUM': status,
            'AI_AÃ‡IKLAMA': explanation,
            'AI_TAVSIYE': ai_tavsiye,
            'YAPTIRIM_RISKI': risk_level,
            'TESPIT_EDILEN_GTIPLER': ', '.join(crawl_result['gtip_codes'][:5]),
            'YAPTIRIMLI_GTIPLER': ', '.join(sanctioned_gtips),
            'ULKE_BAGLANTISI': 'EVET' if crawl_result['country_found'] else 'HAYIR',
            'BAÅLIK': search_result['title'],
            'URL': search_result['url'],
            'Ã–ZET': search_result['snippet'],
            'STATUS_CODE': crawl_result.get('status_code', 'N/A'),
            'KAYNAK_URL': search_result['url'],
            'GÃœVEN_SEVÄ°YESÄ°': f"%{confidence}",
            'NEDENLER': ' | '.join(reasons) if reasons else 'Belirsiz',
            'ARAMA_MOTORU': search_result.get('search_engine', 'bilinmiyor')
        }

def create_detailed_excel_report(results, company, country):
    """DetaylÄ± Excel raporu"""
    try:
        filename = f"{company.replace(' ', '_')}_{country}_ticaret_analiz.xlsx"
        
        wb = Workbook()
        ws1 = wb.active
        ws1.title = "Analiz SonuÃ§larÄ±"
        
        headers = [
            'ÅÄ°RKET', 'ÃœLKE', 'DURUM', 'YAPTIRIM_RISKI', 'ULKE_BAGLANTISI',
            'TESPIT_EDILEN_GTIPLER', 'YAPTIRIMLI_GTIPLER', 'GÃœVEN_SEVÄ°YESÄ°', 'NEDENLER',
            'AI_AÃ‡IKLAMA', 'AI_TAVSIYE', 'BAÅLIK', 'URL', 'Ã–ZET', 'ARAMA_MOTORU'
        ]
        
        for col, header in enumerate(headers, 1):
            cell = ws1.cell(row=1, column=col, value=header)
            cell.font = Font(bold=True)
        
        for row, result in enumerate(results, 2):
            ws1.cell(row=row, column=1, value=str(result.get('ÅÄ°RKET', '')))
            ws1.cell(row=row, column=2, value=str(result.get('ÃœLKE', '')))
            ws1.cell(row=row, column=3, value=str(result.get('DURUM', '')))
            ws1.cell(row=row, column=4, value=str(result.get('YAPTIRIM_RISKI', '')))
            ws1.cell(row=row, column=5, value=str(result.get('ULKE_BAGLANTISI', '')))
            ws1.cell(row=row, column=6, value=str(result.get('TESPIT_EDILEN_GTIPLER', '')))
            ws1.cell(row=row, column=7, value=str(result.get('YAPTIRIMLI_GTIPLER', '')))
            ws1.cell(row=row, column=8, value=str(result.get('GÃœVEN_SEVÄ°YESÄ°', '')))
            ws1.cell(row=row, column=9, value=str(result.get('NEDENLER', '')))
            ws1.cell(row=row, column=10, value=str(result.get('AI_AÃ‡IKLAMA', '')))
            ws1.cell(row=row, column=11, value=str(result.get('AI_TAVSIYE', '')))
            ws1.cell(row=row, column=12, value=str(result.get('BAÅLIK', '')))
            ws1.cell(row=row, column=13, value=str(result.get('URL', '')))
            ws1.cell(row=row, column=14, value=str(result.get('Ã–ZET', '')))
            ws1.cell(row=row, column=15, value=str(result.get('ARAMA_MOTORU', '')))
        
        for column in ws1.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws1.column_dimensions[column_letter].width = adjusted_width
        
        wb.save(filename)
        print(f"âœ… Excel raporu oluÅŸturuldu: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ Excel rapor hatasÄ±: {e}")
        return None

def display_results(results, company, country):
    """SonuÃ§larÄ± gÃ¶ster"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š DUCKDUCKGO ANA, GOOGLE FALLBACK ANALÄ°Z SONUÃ‡LARI: {company} â†” {country}")
    print(f"{'='*80}")
    
    if not results:
        print("âŒ Analiz sonucu bulunamadÄ±!")
        return
    
    total_results = len(results)
    high_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'YÃœKSEK'])
    medium_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'ORTA'])
    country_connection_count = len([r for r in results if r.get('ULKE_BAGLANTISI') == 'EVET'])
    
    print(f"\nğŸ“ˆ Ã–ZET:")
    print(f"   â€¢ Toplam SonuÃ§: {total_results}")
    print(f"   â€¢ Ãœlke BaÄŸlantÄ±sÄ±: {country_connection_count}")
    print(f"   â€¢ YÃœKSEK YaptÄ±rÄ±m Riski: {high_risk_count}")
    print(f"   â€¢ ORTA Risk: {medium_risk_count}")
    
    if high_risk_count > 0:
        print(f"\nâš ï¸  KRÄ°TÄ°K YAPTIRIM UYARISI:")
        for result in results:
            if result.get('YAPTIRIM_RISKI') == 'YÃœKSEK':
                print(f"   ğŸ”´ {result.get('BAÅLIK', '')[:60]}...")
                print(f"      YasaklÄ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
    
    for i, result in enumerate(results, 1):
        print(f"\nğŸ” SONUÃ‡ {i}:")
        print(f"   ğŸ“ BaÅŸlÄ±k: {result.get('BAÅLIK', 'N/A')}")
        print(f"   ğŸŒ URL: {result.get('URL', 'N/A')}")
        print(f"   ğŸ¯ Durum: {result.get('DURUM', 'N/A')}")
        print(f"   âš ï¸  YaptÄ±rÄ±m Riski: {result.get('YAPTIRIM_RISKI', 'N/A')}")
        print(f"   ğŸ”— Ãœlke BaÄŸlantÄ±sÄ±: {result.get('ULKE_BAGLANTISI', 'N/A')}")
        print(f"   ğŸ” GTIP KodlarÄ±: {result.get('TESPIT_EDILEN_GTIPLER', 'Yok')}")
        if result.get('YAPTIRIMLI_GTIPLER'):
            print(f"   ğŸš« YasaklÄ± GTIP: {result.get('YAPTIRIMLI_GTIPLER', '')}")
        print(f"   ğŸ“Š GÃ¼ven Seviyesi: {result.get('GÃœVEN_SEVÄ°YESÄ°', 'N/A')}")
        print(f"   ğŸ” Arama Motoru: {result.get('ARAMA_MOTORU', 'N/A')}")
        print(f"   ğŸ“‹ Nedenler: {result.get('NEDENLER', 'N/A')}")
        print(f"   ğŸ’¡ AÃ§Ä±klama: {result.get('AI_AÃ‡IKLAMA', 'N/A')}")
        print(f"   ğŸ’­ Tavsiye: {result.get('AI_TAVSIYE', 'N/A')}")
        print(f"   {'â”€'*60}")

def main():
    print("ğŸ“Š DUCKDUCKGO ANA, GOOGLE FALLBACK Ä°LE TÄ°CARET ANALÄ°Z SÄ°STEMÄ°")
    print("ğŸ¯ HEDEF: DuckDuckGo ile gÃ¼venilir, Google fallback ile yedekli analiz")
    print("ğŸ’¡ AVANTAJ: JSON hatalarÄ±ndan kaÃ§Ä±nma, kesintisiz Ã§alÄ±ÅŸma")
    print("ğŸ”‘ Google API Key: AIzaSyC2A3ANshAolgr4hNNlFOtgNSlcQtIP40Y")
    print("ğŸ” Google CSE ID: d65dec7934a544da1")
    print("ğŸ¦† Ana Arama: DuckDuckGo\n")
    
    config = Config()
    analyzer = EnhancedTradeAnalyzer(config)
    
    company = input("Åirket adÄ±nÄ± girin: ").strip()
    country = input("Ãœlke adÄ±nÄ± girin: ").strip()
    
    if not company or not country:
        print("âŒ Åirket ve Ã¼lke bilgisi gereklidir!")
        return
    
    print(f"\nğŸš€ DUCKDUCKGO ANA, GOOGLE FALLBACK ANALÄ°ZÄ° BAÅLATILIYOR: {company} â†” {country}")
    print("â³ DuckDuckGo ile arama yapÄ±lÄ±yor...")
    print("   Google API fallback hazÄ±r...")
    print("   Sayfalar analiz ediliyor...")
    print("   GTIP kodlarÄ± taranÄ±yor...")
    print("   YaptÄ±rÄ±m kontrolÃ¼ yapÄ±lÄ±yor...\n")
    
    start_time = time.time()
    results = analyzer.enhanced_analyze(company, country)
    execution_time = time.time() - start_time
    
    if results:
        display_results(results, company, country)
        
        filename = create_detailed_excel_report(results, company, country)
        
        if filename:
            print(f"\nâœ… Excel raporu oluÅŸturuldu: {filename}")
            print(f"â±ï¸  Toplam Ã§alÄ±ÅŸma sÃ¼resi: {execution_time:.2f} saniye")
            
            try:
                open_excel = input("\nğŸ“‚ Excel dosyasÄ±nÄ± aÃ§mak ister misiniz? (e/h): ").strip().lower()
                if open_excel == 'e':
                    if os.name == 'nt':
                        os.system(f'start excel "{filename}"')
                    elif os.name == 'posix':
                        os.system(f'open "{filename}"' if sys.platform == 'darwin' else f'xdg-open "{filename}"')
                    print("ğŸ“‚ Excel dosyasÄ± aÃ§Ä±lÄ±yor...")
            except Exception as e:
                print(f"âš ï¸  Dosya otomatik aÃ§Ä±lamadÄ±: {e}")
                print(f"ğŸ“ Manuel aÃ§Ä±n: {filename}")
        else:
            print("âŒ Excel raporu oluÅŸturulamadÄ±!")
    else:
        print("âŒ Analiz sonucu bulunamadÄ±!")

if __name__ == "__main__":
    main()
