from flask import Flask, request, jsonify, render_template, send_file
import requests
from bs4 import BeautifulSoup
import time
import random
import re
from openpyxl import Workbook
from openpyxl.styles import Font
import sys
import logging
import os
from datetime import datetime
import cloudscraper

app = Flask(__name__)

print("ğŸš€ GELÄ°ÅMÄ°Å TÄ°CARET ANALÄ°Z SÄ°STEMÄ° BAÅLATILIYOR...")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class Config:
    def __init__(self):
        self.MAX_RESULTS = 3
        self.REQUEST_TIMEOUT = 30
        self.RETRY_ATTEMPTS = 2
        self.MAX_GTIP_CHECK = 3
        self.USER_AGENTS = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        ]

class AdvancedCrawler:
    def __init__(self, config):
        self.config = config
        # Cloudscraper ile bot korumasÄ± aÅŸma
        self.scraper = cloudscraper.create_scraper()
    
    def advanced_crawl(self, url, target_country):
        """GeliÅŸmiÅŸ crawl - cloudscraper ile"""
        logging.info(f"ğŸŒ Crawl: {url[:60]}...")
        
        # Domain'e Ã¶zel bekleme
        domain = self._extract_domain(url)
        if any(site in domain for site in ['trademo.com', 'volza.com', 'eximpedia.app']):
            wait_time = random.uniform(5, 8)  # Daha uzun bekleme
            logging.info(f"â³ {domain} iÃ§in {wait_time:.1f}s bekleme...")
            time.sleep(wait_time)
        
        # Ã–nce cloudscraper ile dene
        page_result = self._try_cloudscraper_crawl(url, target_country)
        if page_result['status_code'] == 200:
            return page_result
        
        # Cloudscraper baÅŸarÄ±sÄ±zsa, normal requests ile dene
        page_result = self._try_page_crawl(url, target_country)
        if page_result['status_code'] == 200:
            return page_result
        
        # Her ikisi de baÅŸarÄ±sÄ±zsa snippet analizi
        logging.info(f"ğŸ” Snippet derinlemesine analiz: {url}")
        snippet_analysis = self.analyze_snippet_deep("", target_country, url)
        return {
            'country_found': snippet_analysis['country_found'], 
            'gtip_codes': snippet_analysis['gtip_codes'],
            'content_preview': 'Snippet analizi yapÄ±ldÄ±',
            'status_code': 'SNIPPET_ANALYSIS'
        }
    
    def _try_cloudscraper_crawl(self, url, target_country):
        """Cloudscraper ile crawl dene"""
        try:
            logging.info(f"â˜ï¸ Cloudscraper ile deneme: {url}")
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'DNT': '1',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # Ã–zel domain'ler iÃ§in headers
            domain = self._extract_domain(url)
            if 'trademo.com' in domain:
                headers.update({
                    'Referer': 'https://www.trademo.com/',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1',
                })
            elif 'volza.com' in domain:
                headers.update({
                    'Referer': 'https://www.volza.com/',
                    'Sec-Fetch-Dest': 'document',
                })
            elif 'eximpedia.app' in domain:
                headers.update({
                    'Referer': 'https://www.eximpedia.app/',
                    'Sec-Fetch-Dest': 'document',
                })
            
            response = self.scraper.get(url, headers=headers, timeout=25)
            
            if response.status_code == 200:
                logging.info(f"âœ… Cloudscraper baÅŸarÄ±lÄ±: {url}")
                return self._parse_advanced_content(response.text, target_country, response.status_code)
            else:
                logging.warning(f"âŒ Cloudscraper hatasÄ± {response.status_code}: {url}")
                return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': response.status_code}
        except Exception as e:
            logging.warning(f"âŒ Cloudscraper hatasÄ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'CLOUDSCRAPER_ERROR'}
    
    def _try_page_crawl(self, url, target_country):
        """Normal requests ile crawl dene"""
        try:
            logging.info(f"ğŸŒ Normal requests ile deneme: {url}")
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'DNT': '1',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # Ã–zel domain'ler iÃ§in headers
            domain = self._extract_domain(url)
            if 'trademo.com' in domain:
                headers.update({
                    'Referer': 'https://www.trademo.com/',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1',
                })
            elif 'volza.com' in domain:
                headers.update({
                    'Referer': 'https://www.volza.com/',
                    'Sec-Fetch-Dest': 'document',
                })
            elif 'eximpedia.app' in domain:
                headers.update({
                    'Referer': 'https://www.eximpedia.app/',
                    'Sec-Fetch-Dest': 'document',
                })
            
            response = requests.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                logging.info(f"âœ… Normal requests baÅŸarÄ±lÄ±: {url}")
                return self._parse_advanced_content(response.text, target_country, response.status_code)
            elif response.status_code == 403:
                logging.warning(f"ğŸ”’ 403 Forbidden: {url}")
                return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 403}
            else:
                logging.warning(f"âŒ Sayfa hatasÄ± {response.status_code}: {url}")
                return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': response.status_code}
        except requests.exceptions.Timeout:
            logging.warning(f"â° Timeout: {url}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'TIMEOUT'}
        except Exception as e:
            logging.error(f"âŒ Sayfa crawl hatasÄ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'ERROR'}
    
    def _parse_advanced_content(self, html, target_country, status_code):
        """GeliÅŸmiÅŸ iÃ§erik analizi"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            text_content = soup.get_text()
            text_lower = text_content.lower()
            
            # GeliÅŸmiÅŸ Ã¼lke kontrolÃ¼
            country_found = self._check_country_advanced(text_lower, target_country)
            
            # GeliÅŸmiÅŸ GTIP kod Ã§Ä±karma
            gtip_codes = self.extract_advanced_gtip_codes(text_content)
            
            logging.info(f"ğŸ” Sayfa analizi: Ãœlke={country_found}, GTIP={gtip_codes[:5]}")
            
            return {
                'country_found': country_found,
                'gtip_codes': gtip_codes,
                'content_preview': text_content[:300] + "..." if len(text_content) > 300 else text_content,
                'status_code': status_code
            }
        except Exception as e:
            logging.error(f"âŒ Parse hatasÄ±: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': '', 'status_code': 'PARSE_ERROR'}
    
    def _check_country_advanced(self, text_lower, target_country):
        """GeliÅŸmiÅŸ Ã¼lke kontrolÃ¼"""
        country_variations = [
            target_country.lower(),
            target_country.upper(), 
            target_country.title(),
            'russia', 'rusya', 'rusian', 'rus'  # Rusya iÃ§in Ã¶zel varyasyonlar
        ]
        
        # Ticaret terimleri ile birlikte kontrol
        trade_terms = ['export', 'import', 'country', 'origin', 'destination', 'trade']
        
        for country_var in country_variations:
            if country_var in text_lower:
                # Ãœlke ismi geÃ§iyorsa, ticaret terimleriyle yakÄ±nlÄ±k kontrolÃ¼
                for term in trade_terms:
                    if f"{term} {country_var}" in text_lower or f"{country_var} {term}" in text_lower:
                        logging.info(f"âœ… Ãœlke baÄŸlantÄ±sÄ± bulundu: {term} {country_var}")
                        return True
                logging.info(f"âœ… Ãœlke baÄŸlantÄ±sÄ± bulundu: {country_var}")
                return True
        
        return False
    
    def extract_advanced_gtip_codes(self, text):
        """GeliÅŸmiÅŸ GTIP kod Ã§Ä±karma"""
        patterns = [
            r'\b\d{4}\.?\d{0,4}\b',  # 8708.30 gibi
            r'\b\d{6}\b',  # 870830 gibi
            r'\bHS\s?CODE\s?:?\s?(\d{4,8})\b',
            r'\bGTIP\s?:?\s?(\d{4,8})\b',
            r'\bH\.S\.\s?CODE?\s?:?\s?(\d{4,8})\b',
            r'\bHarmonized\s?System\s?Code\s?:?\s?(\d{4,8})\b',
        ]
        
        all_codes = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                
                # NoktayÄ± kaldÄ±r ve ilk 4 haneyi al
                code = re.sub(r'[^\d]', '', match)
                if len(code) >= 4:
                    all_codes.add(code[:4])
                    logging.info(f"ğŸ” GTIP kodu bulundu: {code[:4]} (orijinal: {match})")
        
        return list(all_codes)
    
    def analyze_snippet_deep(self, snippet_text, target_country, url=""):
        """Snippet derinlemesine analizi - URL'den domain kontrolÃ¼"""
        domain = self._extract_domain(url)
        combined_text = f"{snippet_text} {domain}".lower()
        
        logging.info(f"ğŸ” Snippet analizi: {snippet_text[:100]}...")
        
        # GeliÅŸmiÅŸ Ã¼lke kontrolÃ¼
        country_found = self._check_country_advanced(combined_text, target_country)
        
        # GeliÅŸmiÅŸ GTIP Ã§Ä±karma
        gtip_codes = self.extract_advanced_gtip_codes(snippet_text)
        
        # Domain'e Ã¶zel pattern'ler
        if 'trademo.com' in domain:
            logging.info("ğŸ¯ Trademo.com Ã¶zel analizi...")
            special_patterns = [
                (r'country of export.*russia', 'russia', 'country_found'),
                (r'export.*russia', 'russia', 'country_found'), 
                (r'hs code.*8708', '8708', 'gtip'),
                (r'8708.*hs code', '8708', 'gtip'),
                (r'870830', '8708', 'gtip'),
                (r'8708\.30', '8708', 'gtip'),
                (r'trademo', 'trademo', 'domain_trust'),  # Trademo domain gÃ¼veni
            ]
            
            for pattern, value, pattern_type in special_patterns:
                if re.search(pattern, combined_text, re.IGNORECASE):
                    if pattern_type == 'gtip' and value not in gtip_codes:
                        gtip_codes.append(value)
                        logging.info(f"ğŸ” Trademo Ã¶zel pattern GTIP: {value}")
                    elif pattern_type == 'country_found' and not country_found:
                        country_found = True
                        logging.info(f"ğŸ” Trademo Ã¶zel pattern Ã¼lke: {value}")
        
        logging.info(f"ğŸ” Snippet analizi sonucu: Ãœlke={country_found}, GTIP={gtip_codes}")
        
        return {
            'country_found': country_found,
            'gtip_codes': gtip_codes
        }
    
    def _extract_domain(self, url):
        """URL'den domain Ã§Ä±kar"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""

class EnhancedSearcher:
    def __init__(self, config):
        self.config = config
        self.crawler = AdvancedCrawler(config)
    
    def enhanced_search(self, query, max_results=3):
        """GeliÅŸmiÅŸ arama - cloudscraper ile"""
        try:
            logging.info(f"ğŸ” DuckDuckGo: {query}")
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            }
            
            url = "https://html.duckduckgo.com/html/"
            data = {'q': query, 'b': '', 'kl': 'us-en'}
            
            # Uzun bekleme
            wait_time = random.uniform(3, 6)
            logging.info(f"â³ Arama Ã¶ncesi {wait_time:.1f}s bekleme...")
            time.sleep(wait_time)
            
            # Cloudscraper ile arama yap
            scraper = cloudscraper.create_scraper()
            response = scraper.post(url, data=data, headers=headers, timeout=25)
            
            if response.status_code == 200:
                results = self.parse_enhanced_results(response.text, max_results)
                logging.info(f"âœ… {len(results)} sonuÃ§ bulundu")
                return results
            else:
                logging.warning(f"âŒ DuckDuckGo hatasÄ±: {response.status_code}")
                return []
        except Exception as e:
            logging.error(f"âŒ Arama hatasÄ±: {e}")
            return []
    
    def parse_enhanced_results(self, html, max_results):
        """GeliÅŸmiÅŸ sonuÃ§ parse"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        for div in soup.find_all('div', class_='result')[:max_results]:
            try:
                title_elem = div.find('a', class_='result__a')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                url = title_elem.get('href')
                
                if url and '//duckduckgo.com/l/' in url:
                    try:
                        time.sleep(2)
                        scraper = cloudscraper.create_scraper()
                        redirect_response = scraper.get(url, timeout=10, allow_redirects=True)
                        url = redirect_response.url
                    except:
                        pass
                
                snippet_elem = div.find('a', class_='result__snippet')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                if url and url.startswith('//'):
                    url = 'https:' + url
                
                results.append({
                    'title': title,
                    'url': url,
                    'snippet': snippet,
                    'full_text': f"{title} {snippet}",
                    'domain': self._extract_domain(url)
                })
                
                logging.info(f"ğŸ“„ Bulunan: {title[:60]}...")
                logging.info(f"ğŸŒ Domain: {self._extract_domain(url)}")
                
            except Exception as e:
                logging.error(f"âŒ SonuÃ§ parse hatasÄ±: {e}")
                continue
        
        return results
    
    def _extract_domain(self, url):
        """URL'den domain Ã§Ä±kar"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""

class QuickEURLexChecker:
    def __init__(self, config):
        self.config = config
        self.sanction_cache = {}
    
    def quick_check_gtip(self, gtip_codes):
        """HÄ±zlÄ± GTIP kontrolÃ¼"""
        if not gtip_codes:
            return []
            
        sanctioned_codes = []
        checked_codes = gtip_codes[:self.config.MAX_GTIP_CHECK]
        
        logging.info(f"ğŸ” EUR-Lex kontrolÃ¼: {checked_codes}")
        
        for gtip_code in checked_codes:
            if gtip_code in self.sanction_cache:
                if self.sanction_cache[gtip_code]:
                    sanctioned_codes.append(gtip_code)
                    logging.info(f"â›” Ã–nbellekten yaptÄ±rÄ±mlÄ±: {gtip_code}")
                continue
                
            try:
                wait_time = random.uniform(2, 4)
                logging.info(f"â³ EUR-Lex Ã¶ncesi {wait_time:.1f}s bekleme...")
                time.sleep(wait_time)
                
                url = "https://eur-lex.europa.eu/search.html"
                params = {
                    'text': f'"{gtip_code}" sanction prohibited',
                    'type': 'advanced',
                    'lang': 'en'
                }
                
                headers = {
                    'User-Agent': random.choice(self.config.USER_AGENTS),
                }
                
                response = requests.get(url, params=params, headers=headers, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content = soup.get_text().lower()
                    
                    sanction_terms = ['prohibited', 'banned', 'sanction', 'restricted']
                    found_sanction = any(term in content for term in sanction_terms)
                    
                    if found_sanction:
                        sanctioned_codes.append(gtip_code)
                        self.sanction_cache[gtip_code] = True
                        logging.info(f"â›” YaptÄ±rÄ±mlÄ± kod: {gtip_code}")
                    else:
                        self.sanction_cache[gtip_code] = False
                        logging.info(f"âœ… Kod temiz: {gtip_code}")
                else:
                    logging.warning(f"âŒ EUR-Lex hatasÄ±: {response.status_code}")
                
            except Exception as e:
                logging.error(f"âŒ EUR-Lex kontrol hatasÄ±: {e}")
                continue
        
        return sanctioned_codes

class EnhancedTradeAnalyzer:
    def __init__(self, config):
        self.config = config
        self.searcher = EnhancedSearcher(config)
        self.crawler = AdvancedCrawler(config)
        self.eur_lex_checker = QuickEURLexChecker(config)
    
    def enhanced_analyze(self, company, country):
        """GeliÅŸmiÅŸ analiz - cloudscraper ile"""
        logging.info(f"ğŸ¤– GELÄ°ÅMÄ°Å ANALÄ°Z BAÅLATILIYOR: {company} â†” {country}")
        
        search_queries = [
            f"{company} {country} export",
            f"{company} {country} business",
            f"{company} {country} HS code",
            f"{company} {country} trade"
        ]
        
        all_results = []
        
        for i, query in enumerate(search_queries, 1):
            try:
                logging.info(f"ğŸ” Sorgu {i}/4: {query}")
                
                # Sorgular arasÄ± uzun bekleme
                if i > 1:
                    wait_time = random.uniform(5, 10)
                    logging.info(f"â³ Sorgular arasÄ± {wait_time:.1f}s bekleme...")
                    time.sleep(wait_time)
                
                search_results = self.searcher.enhanced_search(query, self.config.MAX_RESULTS)
                
                if not search_results:
                    logging.warning(f"âš ï¸ Bu sorgu iÃ§in sonuÃ§ bulunamadÄ±: {query}")
                    continue
                
                for j, result in enumerate(search_results, 1):
                    logging.info(f"ğŸ“„ SonuÃ§ {j} analiz ediliyor: {result['title'][:50]}...")
                    
                    # SonuÃ§lar arasÄ± uzun bekleme
                    if j > 1:
                        wait_time = random.uniform(3, 6)
                        logging.info(f"â³ SonuÃ§lar arasÄ± {wait_time:.1f}s bekleme...")
                        time.sleep(wait_time)
                    
                    # GeliÅŸmiÅŸ crawl (cloudscraper ile)
                    crawl_result = self.crawler.advanced_crawl(result['url'], country)
                    
                    # EÄŸer sayfaya eriÅŸilemediyse, snippet derinlemesine analiz
                    if crawl_result['status_code'] not in [200, 403, 'TIMEOUT']:
                        snippet_analysis = self.crawler.analyze_snippet_deep(result['full_text'], country, result['url'])
                        if snippet_analysis['country_found'] or snippet_analysis['gtip_codes']:
                            crawl_result['country_found'] = snippet_analysis['country_found']
                            crawl_result['gtip_codes'] = snippet_analysis['gtip_codes']
                            logging.info(f"ğŸ” Snippet analizi sonucu: Ãœlke={snippet_analysis['country_found']}, GTIP={snippet_analysis['gtip_codes']}")
                    
                    # EUR-Lex kontrolÃ¼
                    sanctioned_gtips = []
                    if crawl_result['gtip_codes']:
                        logging.info(f"ğŸ” EUR-Lex kontrolÃ¼ yapÄ±lÄ±yor...")
                        sanctioned_gtips = self.eur_lex_checker.quick_check_gtip(crawl_result['gtip_codes'])
                    
                    # GÃ¼ven seviyesi hesapla
                    confidence = self._calculate_confidence(crawl_result, sanctioned_gtips, result['domain'])
                    
                    analysis = self.create_enhanced_analysis_result(
                        company, country, result, crawl_result, sanctioned_gtips, confidence
                    )
                    
                    all_results.append(analysis)
                
            except Exception as e:
                logging.error(f"âŒ Sorgu hatasÄ±: {e}")
                continue
        
        return all_results
    
    def _calculate_confidence(self, crawl_result, sanctioned_gtips, domain):
        """GÃ¼ven seviyesi hesapla"""
        confidence = 0
        
        # Domain gÃ¼veni
        trusted_domains = ['trademo.com', 'eximpedia.app', 'volza.com', 'importyet.com', 'emis.com']
        if any(trusted in domain for trusted in trusted_domains):
            confidence += 30
            logging.info(f"ğŸ“Š Domain gÃ¼veni: +30% ({domain})")
        
        # GTIP kodlarÄ±
        if crawl_result['gtip_codes']:
            confidence += 25
            logging.info(f"ğŸ“Š GTIP gÃ¼veni: +25% ({len(crawl_result['gtip_codes'])} kod)")
        
        # Ãœlke baÄŸlantÄ±sÄ±
        if crawl_result['country_found']:
            confidence += 25
            logging.info(f"ğŸ“Š Ãœlke baÄŸlantÄ±sÄ± gÃ¼veni: +25%")
        
        # YaptÄ±rÄ±m tespiti
        if sanctioned_gtips:
            confidence += 20
            logging.info(f"ğŸ“Š YaptÄ±rÄ±m tespiti gÃ¼veni: +20% ({len(sanctioned_gtips)} yaptÄ±rÄ±mlÄ± kod)")
        
        final_confidence = min(confidence, 100)
        logging.info(f"ğŸ“Š Toplam gÃ¼ven seviyesi: %{final_confidence}")
        
        return final_confidence
    
    def create_enhanced_analysis_result(self, company, country, search_result, crawl_result, sanctioned_gtips, confidence):
        """GeliÅŸmiÅŸ analiz sonucu"""
        
        reasons = []
        if crawl_result['country_found']:
            reasons.append("Ãœlke baÄŸlantÄ±sÄ± tespit edildi")
        if crawl_result['gtip_codes']:
            reasons.append(f"GTIP kodlarÄ± bulundu: {', '.join(crawl_result['gtip_codes'][:3])}")
        if sanctioned_gtips:
            reasons.append(f"YaptÄ±rÄ±mlÄ± GTIP kodlarÄ±: {', '.join(sanctioned_gtips)}")
        if any(trusted in search_result['domain'] for trusted in ['trademo.com', 'volza.com', 'eximpedia.app']):
            reasons.append("GÃ¼venilir ticaret verisi kaynaÄŸÄ±")
        
        if sanctioned_gtips:
            status = "YAPTIRIMLI_YÃœKSEK_RISK"
            explanation = f"â›” YÃœKSEK RÄ°SK: {company} ÅŸirketi {country} ile yaptÄ±rÄ±mlÄ± Ã¼rÃ¼n ticareti yapÄ±yor"
            ai_tavsiye = f"â›” ACÄ°L DURUM! Bu Ã¼rÃ¼nlerin {country.upper()} ihracÄ± YASAKTIR: {', '.join(sanctioned_gtips)}"
            risk_level = "YÃœKSEK"
        elif crawl_result['country_found'] and crawl_result['gtip_codes']:
            status = "RISK_VAR"
            explanation = f"ğŸŸ¡ RÄ°SK VAR: {company} ÅŸirketi {country} ile ticaret baÄŸlantÄ±sÄ± bulundu"
            ai_tavsiye = f"Ticaret baÄŸlantÄ±sÄ± doÄŸrulandÄ±. GTIP kodlarÄ±: {', '.join(crawl_result['gtip_codes'][:3])}"
            risk_level = "ORTA"
        elif crawl_result['country_found']:
            status = "Ä°LÄ°ÅKÄ°_VAR"
            explanation = f"ğŸŸ¢ Ä°LÄ°ÅKÄ° VAR: {company} ÅŸirketi {country} ile baÄŸlantÄ±lÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulundu ancak GTIP kodu tespit edilemedi"
            risk_level = "DÃœÅÃœK"
        else:
            status = "TEMIZ"
            explanation = f"âœ… TEMÄ°Z: {company} ÅŸirketinin {country} ile ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            ai_tavsiye = "Ticaret baÄŸlantÄ±sÄ± bulunamadÄ±"
            risk_level = "YOK"
        
        return {
            'ÅÄ°RKET': company,
            'ÃœLKE': country,
            'DURUM': status,
            'AI_AÃ‡IKLAMA': explanation,
            'AI_TAVSIYE': ai_tavsiye,
            'YAPTIRIM_RISKI': risk_level,
            'TESPIT_EDILEN_GTIPLER': ', '.join(crawl_result['gtip_codes'][:5]),
            'YAPTIRIMLI_GTIPLER': ', '.join(sanctioned_gtips),
            'ULKE_BAGLANTISI': 'EVET' if crawl_result['country_found'] else 'HAYIR',
            'BAÅLIK': search_result['title'],
            'URL': search_result['url'],
            'Ã–ZET': search_result['snippet'],
            'STATUS_CODE': crawl_result.get('status_code', 'N/A'),
            'KAYNAK_URL': search_result['url'],
            'GÃœVEN_SEVÄ°YESÄ°': f"%{confidence}",
            'NEDENLER': ' | '.join(reasons) if reasons else 'Belirsiz'
        }

# Kalan fonksiyonlar aynÄ± kalacak...
def create_detailed_excel_report(results, company, country):
    """DetaylÄ± Excel raporu"""
    try:
        filename = f"{company.replace(' ', '_')}_{country}_ticaret_analiz.xlsx"
        filepath = os.path.join('/tmp', filename)
        
        wb = Workbook()
        
        # 1. Sayfa: Analiz SonuÃ§larÄ±
        ws1 = wb.active
        ws1.title = "Analiz SonuÃ§larÄ±"
        
        headers = [
            'ÅÄ°RKET', 'ÃœLKE', 'DURUM', 'YAPTIRIM_RISKI', 'ULKE_BAGLANTISI',
            'TESPIT_EDILEN_GTIPLER', 'YAPTIRIMLI_GTIPLER', 'GÃœVEN_SEVÄ°YESÄ°', 'NEDENLER',
            'AI_AÃ‡IKLAMA', 'AI_TAVSIYE', 'BAÅLIK', 'URL', 'Ã–ZET'
        ]
        
        for col, header in enumerate(headers, 1):
            cell = ws1.cell(row=1, column=col, value=header)
            cell.font = Font(bold=True)
        
        for row, result in enumerate(results, 2):
            ws1.cell(row=row, column=1, value=str(result.get('ÅÄ°RKET', '')))
            ws1.cell(row=row, column=2, value=str(result.get('ÃœLKE', '')))
            ws1.cell(row=row, column=3, value=str(result.get('DURUM', '')))
            ws1.cell(row=row, column=4, value=str(result.get('YAPTIRIM_RISKI', '')))
            ws1.cell(row=row, column=5, value=str(result.get('ULKE_BAGLANTISI', '')))
            ws1.cell(row=row, column=6, value=str(result.get('TESPIT_EDILEN_GTIPLER', '')))
            ws1.cell(row=row, column=7, value=str(result.get('YAPTIRIMLI_GTIPLER', '')))
            ws1.cell(row=row, column=8, value=str(result.get('GÃœVEN_SEVÄ°YESÄ°', '')))
            ws1.cell(row=row, column=9, value=str(result.get('NEDENLER', '')))
            ws1.cell(row=row, column=10, value=str(result.get('AI_AÃ‡IKLAMA', '')))
            ws1.cell(row=row, column=11, value=str(result.get('AI_TAVSIYE', '')))
            ws1.cell(row=row, column=12, value=str(result.get('BAÅLIK', '')))
            ws1.cell(row=row, column=13, value=str(result.get('URL', '')))
            ws1.cell(row=row, column=14, value=str(result.get('Ã–ZET', '')))
        
        # 2. Sayfa: Yapay Zeka Ã–zeti
        ws2 = wb.create_sheet("AI Analiz Ã–zeti")
        
        # BaÅŸlÄ±k
        ws2.merge_cells('A1:H1')
        title_cell = ws2.cell(row=1, column=1, value="YAPAY ZEKA TÄ°CARET ANALÄ°Z YORUMU")
        title_cell.font = Font(bold=True, size=16)
        
        # Åirket ve Ãœlke Bilgisi
        ws2.cell(row=3, column=1, value="ÅÄ°RKET:")
        ws2.cell(row=3, column=2, value=company)
        ws2.cell(row=4, column=1, value="ÃœLKE:")
        ws2.cell(row=4, column=2, value=country)
        ws2.cell(row=5, column=1, value="ANALÄ°Z TARÄ°HÄ°:")
        ws2.cell(row=5, column=2, value=datetime.now().strftime("%d/%m/%Y %H:%M"))
        
        # Ã–zet Bilgiler
        ws2.cell(row=7, column=1, value="TOPLAM SONUÃ‡:")
        ws2.cell(row=7, column=2, value=len(results))
        
        high_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'YÃœKSEK'])
        medium_risk_count = len([r for r in results if r.get('YAPTIRIM_RISKI') == 'ORTA'])
        country_connection_count = len([r for r in results if r.get('ULKE_BAGLANTISI') == 'EVET'])
        
        ws2.cell(row=8, column=1, value="YÃœKSEK RÄ°SK:")
        ws2.cell(row=8, column=2, value=high_risk_count)
        ws2.cell(row=9, column=1, value="ORTA RÄ°SK:")
        ws2.cell(row=9, column=2, value=medium_risk_count)
        ws2.cell(row=10, column=1, value="ÃœLKE BAÄLANTISI:")
        ws2.cell(row=10, column=2, value=country_connection_count)
        
        # Ortalama GÃ¼ven Seviyesi
        avg_confidence = 0
        confidence_values = [int(r.get('GÃœVEN_SEVÄ°YESÄ°', '0%').strip('%')) for r in results if r.get('GÃœVEN_SEVÄ°YESÄ°')]
        if confidence_values:
            avg_confidence = sum(confidence_values) // len(confidence_values)
        
        ws2.cell(row=11, column=1, value="ORTALAMA GÃœVEN:")
        ws2.cell(row=11, column=2, value=f"%{avg_confidence}")
        
        # Yapay Zeka Yorumu
        ws2.cell(row=13, column=1, value="YAPAY ZEKA ANALÄ°Z YORUMU:").font = Font(bold=True)
        
        if high_risk_count > 0:
            yorum = f"KRÄ°TÄ°K RÄ°SK! {company} ÅŸirketinin {country} ile yaptÄ±rÄ±mlÄ± Ã¼rÃ¼n ticareti tespit edildi. "
            yorum += f"Toplam {high_risk_count} farklÄ± kaynakta yaptÄ±rÄ±mlÄ± GTIP kodlarÄ± bulundu. "
            yorum += f"Ortalama gÃ¼ven seviyesi: %{avg_confidence}. Acil Ã¶nlem alÄ±nmasÄ± gerekmektedir."
        elif medium_risk_count > 0:
            yorum = f"ORTA RÄ°SK! {company} ÅŸirketinin {country} ile ticaret baÄŸlantÄ±sÄ± bulundu. "
            yorum += f"{medium_risk_count} farklÄ± kaynakta ticaret iliÅŸkisi doÄŸrulandÄ±. "
            yorum += f"Ortalama gÃ¼ven seviyesi: %{avg_confidence}. DetaylÄ± inceleme Ã¶nerilir."
        elif country_connection_count > 0:
            yorum = f"DÃœÅÃœK RÄ°SK! {company} ÅŸirketinin {country} ile baÄŸlantÄ±sÄ± bulundu ancak yaptÄ±rÄ±m riski tespit edilmedi. "
            yorum += f"Ortalama gÃ¼ven seviyesi: %{avg_confidence}. Standart ticaret prosedÃ¼rleri uygulanabilir."
        else:
            yorum = f"TEMÄ°Z! {company} ÅŸirketinin {country} ile ticaret baÄŸlantÄ±sÄ± bulunamadÄ±. "
            yorum += f"Ortalama gÃ¼ven seviyesi: %{avg_confidence}. Herhangi bir yaptÄ±rÄ±m riski tespit edilmedi."
        
        ws2.cell(row=14, column=1, value=yorum)
        
        # Tavsiyeler
        ws2.cell(row=16, column=1, value="YAPAY ZEKA TAVSÄ°YELERÄ°:").font = Font(bold=True)
        
        if high_risk_count > 0:
            tavsiye = "1. YaptÄ±rÄ±mlÄ± Ã¼rÃ¼n ihracÄ±ndan acilen kaÃ§Ä±nÄ±n\n"
            tavsiye += "2. Yasal danÄ±ÅŸmanla gÃ¶rÃ¼ÅŸÃ¼n\n"
            tavsiye += "3. Ticaret partnerlerini yeniden deÄŸerlendirin\n"
            tavsiye += "4. Uyum birimini bilgilendirin"
        elif medium_risk_count > 0:
            tavsiye = "1. DetaylÄ± due diligence yapÄ±n\n"
            tavsiye += "2. Ticaret dokÃ¼manlarÄ±nÄ± kontrol edin\n"
            tavsiye += "3. GÃ¼ncel yaptÄ±rÄ±m listelerini takip edin\n"
            tavsiye += "4. Alternatif pazarlarÄ± deÄŸerlendirin"
        else:
            tavsiye = "1. Standart ticaret prosedÃ¼rlerine devam edin\n"
            tavsiye += "2. Pazar araÅŸtÄ±rmalarÄ±nÄ± sÃ¼rdÃ¼rÃ¼n\n"
            tavsiye += "3. DÃ¼zenli olarak kontrol edin\n"
            tavsiye += "4. Yeni iÅŸ fÄ±rsatlarÄ±nÄ± deÄŸerlendirin"
        
        ws2.cell(row=17, column=1, value=tavsiye)
        
        # Stil ayarlarÄ±
        for column in ws1.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws1.column_dimensions[column_letter].width = adjusted_width
        
        ws2.column_dimensions['A'].width = 25
        ws2.column_dimensions['B'].width = 50
        
        wb.save(filepath)
        logging.info(f"âœ… DetaylÄ± Excel raporu oluÅŸturuldu: {filepath}")
        return filepath
        
    except Exception as e:
        logging.error(f"âŒ Excel rapor oluÅŸturma hatasÄ±: {e}")
        return None

# Flask Route'larÄ±
@app.route('/')
def home():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        start_time = time.time()
        
        data = request.get_json()
        company = data.get('company', '').strip()
        country = data.get('country', '').strip()
        
        if not company or not country:
            return jsonify({"error": "Åirket ve Ã¼lke bilgisi gereklidir"}), 400
        
        logging.info(f"ğŸš€ GELÄ°ÅMÄ°Å ANALÄ°Z BAÅLATILIYOR: {company} - {country}")
        
        config = Config()
        analyzer = EnhancedTradeAnalyzer(config)
        
        results = analyzer.enhanced_analyze(company, country)
        
        excel_filepath = create_detailed_excel_report(results, company, country)
        
        execution_time = time.time() - start_time
        
        response_data = {
            "success": True,
            "company": company,
            "country": country,
            "execution_time": f"{execution_time:.2f}s",
            "total_results": len(results),
            "analysis": results,
            "excel_download_url": f"/download-excel?company={company}&country={country}" if excel_filepath else None
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"âŒ Analiz hatasÄ±: {e}")
        return jsonify({"error": f"Sunucu hatasÄ±: {str(e)}"}), 500

@app.route('/download-excel')
def download_excel():
    try:
        company = request.args.get('company', '')
        country = request.args.get('country', '')
        
        filename = f"{company.replace(' ', '_')}_{country}_ticaret_analiz.xlsx"
        filepath = os.path.join('/tmp', filename)
        
        if os.path.exists(filepath):
            return send_file(
                filepath,
                as_attachment=True,
                download_name=filename,
                mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
        else:
            return jsonify({"error": "Excel dosyasÄ± bulunamadÄ±"}), 404
            
    except Exception as e:
        logging.error(f"âŒ Excel indirme hatasÄ±: {e}")
        return jsonify({"error": f"Ä°ndirme hatasÄ±: {str(e)}"}), 500

@app.route('/health')
def health():
    return jsonify({"status": "healthy", "timestamp": datetime.now().isoformat()})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
