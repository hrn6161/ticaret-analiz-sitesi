from flask import Flask, request, jsonify, render_template, send_file
import requests
from bs4 import BeautifulSoup
import time
import random
import re
from openpyxl import Workbook
from openpyxl.styles import Font
import sys
import logging
import os
from datetime import datetime

app = Flask(__name__)

print("üöÄ GELƒ∞≈ûMƒ∞≈û CRAWLER ƒ∞LE DUCKDUCKGO ANALƒ∞Z Sƒ∞STEMƒ∞ BA≈ûLATILIYOR...")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class Config:
    def __init__(self):
        self.MAX_RESULTS = 3
        self.REQUEST_TIMEOUT = 30
        self.RETRY_ATTEMPTS = 3
        self.DELAY_BETWEEN_REQUESTS = random.uniform(3, 7)  # Rastgele bekleme 3-7 saniye
        self.DELAY_BETWEEN_SEARCHES = random.uniform(5, 10)  # Rastgele bekleme 5-10 saniye
        self.USER_AGENTS = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/120.0",
        ]

class AdvancedCrawler:
    def __init__(self, config):
        self.config = config
    
    def crawl_with_retry(self, url, target_country, max_retries=3):
        """Sayfayƒ± crawl et - retry mekanizmasƒ± ile"""
        for attempt in range(max_retries):
            try:
                # Rastgele bekleme (anti-bot i√ßin)
                delay = random.uniform(2, 5)
                logging.info(f"‚è≥ {delay:.1f} saniye bekleniyor (attempt {attempt + 1}/{max_retries})...")
                time.sleep(delay)
                
                return self._crawl_page(url, target_country)
                
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Crawl attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    retry_delay = random.uniform(5, 10)
                    logging.info(f"üîÑ {retry_delay:.1f} saniye sonra tekrar denenecek...")
                    time.sleep(retry_delay)
                else:
                    logging.error(f"‚ùå T√ºm crawl attempt'leri ba≈üarƒ±sƒ±z: {e}")
        
        return {'country_found': False, 'gtip_codes': [], 'content_preview': ''}
    
    def _crawl_page(self, url, target_country):
        """Sayfa i√ßeriƒüini crawl et"""
        try:
            if not url or not url.startswith('http'):
                return {'country_found': False, 'gtip_codes': [], 'content_preview': ''}
                
            logging.info(f"üåê Sayfa crawl ediliyor: {url}")
            
            # Rastgele user-agent se√ß
            user_agent = random.choice(self.config.USER_AGENTS)
            
            headers = {
                'User-Agent': user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
            }
            
            # Session kullanarak daha ger√ßek√ßi tarama
            session = requests.Session()
            session.headers.update(headers)
            
            response = session.get(
                url, 
                timeout=self.config.REQUEST_TIMEOUT,
                allow_redirects=True
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Script ve style tag'lerini temizle
                for script in soup(["script", "style", "nav", "header", "footer"]):
                    script.decompose()
                
                # Meta description'ƒ± da al
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                meta_content = meta_desc.get('content', '') if meta_desc else ''
                
                # T√ºm metni al
                text_content = soup.get_text()
                combined_content = f"{meta_content} {text_content}"
                
                # Temizleme
                lines = (line.strip() for line in combined_content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                cleaned_content = ' '.join(chunk for chunk in chunks if chunk)
                
                text_lower = cleaned_content.lower()
                
                # √úlke ismini ara (case insensitive)
                country_found = target_country.lower() in text_lower
                
                # GTIP/HS kodlarƒ±nƒ± ara
                gtip_codes = self.extract_gtip_codes(cleaned_content)
                
                content_preview = cleaned_content[:400] + "..." if len(cleaned_content) > 400 else cleaned_content
                
                logging.info(f"üîç Sayfa analizi: √úlke bulundu={country_found}, GTIP kodlarƒ±={gtip_codes}")
                
                return {
                    'country_found': country_found,
                    'gtip_codes': gtip_codes,
                    'content_preview': content_preview,
                    'status_code': response.status_code
                }
            else:
                logging.warning(f"‚ùå Sayfa crawl hatasƒ±: {response.status_code} - {url}")
                return {
                    'country_found': False, 
                    'gtip_codes': [], 
                    'content_preview': '',
                    'status_code': response.status_code
                }
                
        except Exception as e:
            logging.error(f"‚ùå Crawl hatasƒ± {url}: {e}")
            return {'country_found': False, 'gtip_codes': [], 'content_preview': ''}
    
    def extract_gtip_codes(self, text):
        """Metinden GTIP/HS kodlarƒ±nƒ± √ßƒ±kar"""
        patterns = [
            r'\b\d{4}\.?\d{0,4}\b',
            r'\bHS\s?CODE\s?:?\s?(\d{4,8})\b',
            r'\bHS\s?:?\s?(\d{4,8})\b',
            r'\bGTIP\s?:?\s?(\d{4,8})\b',
            r'\bH\.S\.\s?CODE?\s?:?\s?(\d{4,8})\b',
            r'\bHarmonized System\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bCustoms\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bTariff\s?Code\s?:?\s?(\d{4,8})\b',
            r'\bCN\s?code\s?:?\s?(\d{4,8})\b',
        ]
        
        all_codes = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                
                code = re.sub(r'[^\d]', '', match)
                if len(code) >= 4:
                    main_code = code[:4]
                    if main_code.isdigit():
                        all_codes.add(main_code)
        
        # 4 haneli sayƒ±larƒ± kontrol et (GTIP aralƒ±ƒüƒ±nda mƒ±)
        number_pattern = r'\b\d{4}\b'
        numbers = re.findall(number_pattern, text)
        
        for num in numbers:
            if num.isdigit():
                num_int = int(num)
                # Geni≈ü GTIP aralƒ±klarƒ±
                if ((8400 <= num_int <= 8600) or (8700 <= num_int <= 8900) or 
                    (9000 <= num_int <= 9300) or (2800 <= num_int <= 2900) or
                    (8470 <= num_int <= 8480) or (8500 <= num_int <= 8520) or
                    (8540 <= num_int <= 8550) or (9301 <= num_int <= 9307) or
                    (8701 <= num_int <= 8716) or (8801 <= num_int <= 8807)):
                    all_codes.add(num)
        
        return list(all_codes)

class DuckDuckGoSearcher:
    def __init__(self, config):
        self.config = config
    
    def search_duckduckgo(self, query, max_results=3):
        """DuckDuckGo'da arama yap"""
        try:
            logging.info(f"üîç DuckDuckGo'da aranƒ±yor: {query}")
            
            # Tƒ±rnak i≈üaretlerini kaldƒ±r
            query = query.replace('"', '')
            
            # Rastgele bekleme
            delay = random.uniform(2, 4)
            time.sleep(delay)
            
            headers = {
                'User-Agent': random.choice(self.config.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
            }
            
            # DuckDuckGo HTML arama URL'si
            url = "https://html.duckduckgo.com/html/"
            data = {
                'q': query,
                'b': '',
                'kl': 'us-en'
            }
            
            response = requests.post(
                url, 
                data=data, 
                headers=headers, 
                timeout=self.config.REQUEST_TIMEOUT
            )
            
            if response.status_code == 200:
                return self.parse_duckduckgo_results(response.text, max_results)
            else:
                logging.warning(f"DuckDuckGo arama hatasƒ±: {response.status_code}")
                return []
                
        except Exception as e:
            logging.error(f"DuckDuckGo arama hatasƒ±: {e}")
            return []
    
    def parse_duckduckgo_results(self, html, max_results):
        """DuckDuckGo sonu√ßlarƒ±nƒ± parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        # DuckDuckGo search result div'leri
        result_divs = soup.find_all('div', class_='result')
        
        for div in result_divs[:max_results]:
            try:
                # Ba≈ülƒ±k
                title_elem = div.find('a', class_='result__a')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                
                # URL
                url = title_elem.get('href')
                if url and '//duckduckgo.com/l/' in url:
                    # DuckDuckGo redirect link'ini √ß√∂z
                    try:
                        redirect_response = requests.get(url, timeout=10, allow_redirects=True)
                        url = redirect_response.url
                    except:
                        # Redirect √ß√∂z√ºlemezse orijinal URL'yi kullan
                        pass
                
                # √ñzet
                snippet_elem = div.find('a', class_='result__snippet')
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                
                # URL'yi temizle
                if url and url.startswith('//'):
                    url = 'https:' + url
                
                results.append({
                    'title': title,
                    'url': url,
                    'snippet': snippet
                })
                
                logging.info(f"üìÑ Bulunan sonu√ß: {title[:60]}...")
                
            except Exception as e:
                logging.error(f"Sonu√ß parse hatasƒ±: {e}")
                continue
        
        logging.info(f"‚úÖ DuckDuckGo'dan {len(results)} sonu√ß bulundu")
        return results

class EURLexChecker:
    def __init__(self, config):
        self.config = config
    
    def check_gtip_in_eur_lex(self, gtip_codes):
        """GTIP kodlarƒ±nƒ± EUR-Lex'te kontrol et"""
        sanctioned_codes = []
        
        for gtip_code in gtip_codes:
            try:
                logging.info(f"üîç EUR-Lex'te kontrol ediliyor: GTIP {gtip_code}")
                
                # Rastgele bekleme
                delay = random.uniform(1, 3)
                time.sleep(delay)
                
                # EUR-Lex arama URL'si
                url = "https://eur-lex.europa.eu/search.html"
                params = {
                    'qid': int(time.time()),
                    'text': f'"{gtip_code}" prohibited banned sanction restricted embargo',
                    'type': 'advanced',
                    'lang': 'en'
                }
                
                headers = {
                    'User-Agent': random.choice(self.config.USER_AGENTS),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                }
                
                response = requests.get(url, params=params, headers=headers, timeout=self.config.REQUEST_TIMEOUT)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content = soup.get_text().lower()
                    
                    # Yaptƒ±rƒ±m terimlerini ara
                    sanction_terms = [
                        'prohibited', 'banned', 'sanction', 'restricted', 
                        'embargo', 'forbidden', 'prohibition', 'ban',
                        'not allowed', 'not permitted', 'restriction'
                    ]
                    found_sanction = any(term in content for term in sanction_terms)
                    
                    if found_sanction:
                        sanctioned_codes.append(gtip_code)
                        logging.warning(f"‚õî Yaptƒ±rƒ±mlƒ± kod bulundu: {gtip_code}")
                    else:
                        logging.info(f"‚úÖ Kod temiz: {gtip_code}")
                else:
                    logging.warning(f"EUR-Lex eri≈üim hatasƒ±: {response.status_code}")
                
            except Exception as e:
                logging.error(f"EUR-Lex kontrol hatasƒ± GTIP {gtip_code}: {e}")
                continue
        
        return sanctioned_codes

class AdvancedTradeAnalyzer:
    def __init__(self, config):
        self.config = config
        self.searcher = DuckDuckGoSearcher(config)
        self.crawler = AdvancedCrawler(config)
        self.eur_lex_checker = EURLexChecker(config)
    
    def analyze_company_country(self, company, country):
        """≈ûirket-√ºlke analizi yap"""
        logging.info(f"ü§ñ GELƒ∞≈ûMƒ∞≈û ANALƒ∞Z BA≈ûLATILIYOR: {company} ‚Üî {country}")
        
        search_queries = [
            f"{company} {country} export",
            f"{company} {country} business",
            f"{company} {country} trade",
        ]
        
        all_results = []
        
        for i, query in enumerate(search_queries, 1):
            try:
                logging.info(f"üîç Sorgu {i}/{len(search_queries)}: {query}")
                
                # DuckDuckGo'da ara
                search_results = self.searcher.search_duckduckgo(query, self.config.MAX_RESULTS)
                
                if not search_results:
                    logging.warning(f"‚ùå Bu sorgu i√ßin sonu√ß bulunamadƒ±: {query}")
                    continue
                
                for j, result in enumerate(search_results, 1):
                    logging.info(f"üìÑ Sonu√ß {j} analiz ediliyor: {result['title'][:50]}...")
                    
                    # Sayfayƒ± crawl et (retry ile)
                    crawl_result = self.crawler.crawl_with_retry(result['url'], country)
                    
                    # Eƒüer √ºlke baƒülantƒ±sƒ± bulunduysa ve GTIP kodlarƒ± varsa, EUR-Lex kontrol√º yap
                    sanctioned_gtips = []
                    if crawl_result['country_found'] and crawl_result['gtip_codes']:
                        logging.info(f"üîç EUR-Lex kontrol√º yapƒ±lƒ±yor...")
                        sanctioned_gtips = self.eur_lex_checker.check_gtip_in_eur_lex(crawl_result['gtip_codes'])
                    
                    # Analiz sonucu olu≈ütur
                    analysis = self.create_analysis_result(
                        company, country, result, crawl_result, sanctioned_gtips
                    )
                    
                    all_results.append(analysis)
                
                # Sorgular arasƒ±nda rastgele bekleme
                if i < len(search_queries):
                    delay = random.uniform(5, 10)
                    logging.info(f"‚è≥ {delay:.1f} saniye bekleniyor (sorgular arasƒ±)...")
                    time.sleep(delay)
                
            except Exception as e:
                logging.error(f"‚ùå Sorgu hatasƒ± '{query}': {e}")
                continue
        
        return all_results
    
    def create_analysis_result(self, company, country, search_result, crawl_result, sanctioned_gtips):
        """Analiz sonucu olu≈ütur"""
        
        # Risk deƒüerlendirmesi
        if sanctioned_gtips:
            status = "YAPTIRIMLI_Y√úKSEK_RISK"
            explanation = f"‚õî Y√úKSEK Rƒ∞SK: {company} ≈üirketi {country} ile yaptƒ±rƒ±mlƒ± √ºr√ºn ticareti yapƒ±yor"
            ai_tavsiye = f"‚õî ACƒ∞L DURUM! Bu √ºr√ºnlerin {country.upper()} ihracƒ± YASAKTIR: {', '.join(sanctioned_gtips)}"
            risk_level = "Y√úKSEK"
        elif crawl_result['country_found'] and crawl_result['gtip_codes']:
            status = "RISK_VAR"
            explanation = f"üü° Rƒ∞SK VAR: {company} ≈üirketi {country} ile ticaret baƒülantƒ±sƒ± bulundu"
            ai_tavsiye = f"Ticaret baƒülantƒ±sƒ± doƒürulandƒ±. GTIP kodlarƒ±: {', '.join(crawl_result['gtip_codes'])}"
            risk_level = "ORTA"
        elif crawl_result['country_found']:
            status = "ƒ∞Lƒ∞≈ûKƒ∞_VAR"
            explanation = f"üü¢ ƒ∞Lƒ∞≈ûKƒ∞ VAR: {company} ≈üirketi {country} ile baƒülantƒ±lƒ±"
            ai_tavsiye = "Ticaret baƒülantƒ±sƒ± bulundu ancak GTIP kodu tespit edilemedi"
            risk_level = "D√ú≈û√úK"
        else:
            status = "TEMIZ"
            explanation = f"‚úÖ TEMƒ∞Z: {company} ≈üirketinin {country} ile ticaret baƒülantƒ±sƒ± bulunamadƒ±"
            ai_tavsiye = "Ticaret baƒülantƒ±sƒ± bulunamadƒ±"
            risk_level = "YOK"
        
        return {
            '≈ûƒ∞RKET': company,
            '√úLKE': country,
            'DURUM': status,
            'AI_A√áIKLAMA': explanation,
            'AI_TAVSIYE': ai_tavsiye,
            'YAPTIRIM_RISKI': risk_level,
            'TESPIT_EDILEN_GTIPLER': ', '.join(crawl_result['gtip_codes']),
            'YAPTIRIMLI_GTIPLER': ', '.join(sanctioned_gtips),
            'ULKE_BAGLANTISI': 'EVET' if crawl_result['country_found'] else 'HAYIR',
            'BA≈ûLIK': search_result['title'],
            'URL': search_result['url'],
            '√ñZET': search_result['snippet'],
            'CONTENT_PREVIEW': crawl_result['content_preview'],
            'STATUS_CODE': crawl_result.get('status_code', 'N/A')
        }

def create_excel_report(results, company, country):
    """Excel raporu olu≈ütur"""
    try:
        filename = f"{company.replace(' ', '_')}_{country}_gelismis_analiz.xlsx"
        filepath = os.path.join('/tmp', filename)
        
        wb = Workbook()
        ws = wb.active
        ws.title = "Geli≈ümi≈ü Analiz Sonu√ßlarƒ±"
        
        headers = [
            '≈ûƒ∞RKET', '√úLKE', 'DURUM', 'YAPTIRIM_RISKI', 'ULKE_BAGLANTISI',
            'TESPIT_EDILEN_GTIPLER', 'YAPTIRIMLI_GTIPLER', 'AI_A√áIKLAMA',
            'AI_TAVSIYE', 'BA≈ûLIK', 'URL', '√ñZET', 'STATUS_CODE'
        ]
        
        for col, header in enumerate(headers, 1):
            ws.cell(row=1, column=col, value=header).font = Font(bold=True)
        
        for row, result in enumerate(results, 2):
            ws.cell(row=row, column=1, value=str(result.get('≈ûƒ∞RKET', '')))
            ws.cell(row=row, column=2, value=str(result.get('√úLKE', '')))
            ws.cell(row=row, column=3, value=str(result.get('DURUM', '')))
            ws.cell(row=row, column=4, value=str(result.get('YAPTIRIM_RISKI', '')))
            ws.cell(row=row, column=5, value=str(result.get('ULKE_BAGLANTISI', '')))
            ws.cell(row=row, column=6, value=str(result.get('TESPIT_EDILEN_GTIPLER', '')))
            ws.cell(row=row, column=7, value=str(result.get('YAPTIRIMLI_GTIPLER', '')))
            ws.cell(row=row, column=8, value=str(result.get('AI_A√áIKLAMA', '')))
            ws.cell(row=row, column=9, value=str(result.get('AI_TAVSIYE', '')))
            ws.cell(row=row, column=10, value=str(result.get('BA≈ûLIK', '')))
            ws.cell(row=row, column=11, value=str(result.get('URL', '')))
            ws.cell(row=row, column=12, value=str(result.get('√ñZET', '')))
            ws.cell(row=row, column=13, value=str(result.get('STATUS_CODE', '')))
        
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width
        
        wb.save(filepath)
        logging.info(f"‚úÖ Excel raporu olu≈üturuldu: {filepath}")
        return filepath
        
    except Exception as e:
        logging.error(f"‚ùå Excel rapor olu≈üturma hatasƒ±: {e}")
        return None

# Flask Route'larƒ±
@app.route('/')
def home():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"error": "JSON verisi bekleniyor"}), 400
        
        company = data.get('company', '').strip()
        country = data.get('country', '').strip()
        
        if not company or not country:
            return jsonify({"error": "≈ûirket ve √ºlke bilgisi gereklidir"}), 400
        
        logging.info(f"üöÄ GELƒ∞≈ûMƒ∞≈û ANALƒ∞Z BA≈ûLATILIYOR: {company} - {country}")
        
        config = Config()
        analyzer = AdvancedTradeAnalyzer(config)
        
        # Ger√ßek analiz yap
        results = analyzer.analyze_company_country(company, country)
        
        # Excel raporu olu≈ütur
        excel_filepath = create_excel_report(results, company, country)
        
        response_data = {
            "success": True,
            "company": company,
            "country": country,
            "total_results": len(results),
            "analysis": results,
            "excel_download_url": f"/download-excel?company={company}&country={country}" if excel_filepath else None
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        logging.error(f"‚ùå Analiz hatasƒ±: {e}")
        return jsonify({"error": f"Sunucu hatasƒ±: {str(e)}"}), 500

@app.route('/download-excel')
def download_excel():
    try:
        company = request.args.get('company', '')
        country = request.args.get('country', '')
        
        if not company or not country:
            return jsonify({"error": "≈ûirket ve √ºlke bilgisi gereklidir"}), 400
        
        filename = f"{company.replace(' ', '_')}_{country}_gelismis_analiz.xlsx"
        filepath = os.path.join('/tmp', filename)
        
        if os.path.exists(filepath):
            return send_file(
                filepath,
                as_attachment=True,
                download_name=filename,
                mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
        else:
            return jsonify({"error": "Excel dosyasƒ± bulunamadƒ±"}), 404
            
    except Exception as e:
        logging.error(f"‚ùå Excel indirme hatasƒ±: {e}")
        return jsonify({"error": f"ƒ∞ndirme hatasƒ±: {str(e)}"}), 500

@app.route('/health')
def health():
    return jsonify({"status": "healthy", "timestamp": datetime.now().isoformat()})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
